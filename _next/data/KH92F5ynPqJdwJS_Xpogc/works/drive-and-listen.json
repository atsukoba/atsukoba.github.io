{"pageProps":{"content":"\n<div class=\"iframe-video-wrapper\">\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BlPp1gVSaEQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nWe developed an inference environment for object recognition models using YOLO (v2, v3) (Darknet) and unique OSC sequencer for playing in CPU environment. Then sort (non-maximum suppresion) each unique object recognized to convert into sounds. The player is implemented with MIDI connection between Max and Ableton Live.\n\n[![Image from Gyazo](https://i.gyazo.com/7231b69bed264b915924c4377bc2e2aa.png)](https://gyazo.com/7231b69bed264b915924c4377bc2e2aa)\n\nAtsuya Kobayashi  \nTakumi Inoue  \nKeisuke Okazaki  \nYoriaki Hirota\n","data":{"description":"'Drive and listen' is an experimental work aiming to interpreting cities through sound by trafic sonification.","title":"Drive-and-Listen","slug":"drive-and-listen","date":"2020-07-15T23:20:16.000Z","category":"Prototype","tags":["Sonification"],"keyVisual":"https://i.gyazo.com/3b325bfa825898b7d7d31b95c97c52cf.jpg"}},"__N_SSG":true}