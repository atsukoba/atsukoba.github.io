{"pageProps":{"data":["---\nslug: doc2vec-sansan-blog\ntitle: 「Doc2Vecによる文書ベクトル推論の安定化について」を書きました。\ndate: 2019-04-11 01:08:35\ncategory: \"Tech Blog\"\ntags: [機械学習, 自然言語処理]\nkeyVisual: https://i.gyazo.com/d7530d08e16d37db4336d0f99725bbe9.png\n---\n\n先日までインターンでお世話になっていたSansan株式会社のDSOCという部署で，テックブログを書きましたのでここでも共有しておきます。細かい詳細や追記，追試などもここに今後書くかもしれないです。-> [Doc2Vecによる文書ベクトル推論の安定化について - Sansan Builders Box](https://buildersbox.corp-sansan.com/entry/2019/04/10/110000)\n","---\ntitle: Doc2VecとOptunaを使ったSVMでのテキスト分類を作ってみた\ndescription: \"Optunaを使ってみる練習として，Doc2Vecを用いてテキスト分類をするやつをサクっと書きました。出力はモデルのaccuracy，F1と，`pyplot`でのConfusion Matrixを出力します。\"\nslug: doc2vec-svm-sentenceclassification\ndate: 2019-02-15 23:20:16\ncategory: \"Tech Blog\"\ntags: [機械学習, 自然言語処理]\nkeyVisual: https://i.gyazo.com/2beff33d13697eb79d89a39204d44373.png\n---\n\nOptunaを使ってみる練習として，Doc2Vecを用いてテキスト分類をするやつをサクっと書きました。  \n出力はモデルのaccuracy，F1と，`pyplot`でのConfusion Matrixを出力します。\n\nラベルデータを下記の形で用意します\n\n| DOCUMENT_FILE_NAME(id) | LABEL(labels) |\n|:----------------------:|:-------------:|\n|        foo.txt         |      bar      |\n|        bar.txt         |      foo      |\n\n使い方は，`% python document_SVClassifier.py -h`で。\n\n各テキストデータ毎に分類を行います。\n\n### Source ([github](https://gist.github.com/atsukoba/b33967dee47e92f58240a3a544d0650b))\n\n\n```python\n# Author: Atsuya Kobayashi @atsuya_kobayashi\n# 2019/02/15 17:20\n\n\"\"\"Support Vector Document Classifier with doc2vec & Optuna\n- .csv label file must be in the form of following style\n|DOCUMENT_FILE_NAME(id)|LABEL(labels)|\n|----------------------|-------------|\n|       foo.txt        |     bar     |\n|       bar.txt        |     foo     |\n\"\"\"\n\nimport argparse\nimport itertools\nimport optuna\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.svm import SVC\nfrom gensim.models import Doc2Vec\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n\n# parameters\nPATH_TO_CSVFILE = \"\"\nTEXTFILE_TARGET_DIR = \"/\"\nPATH_TO_PRETRAINED_DOC2VEC_MODEL = \"\"\nN_OPTIMIZE_TRIAL = 20\nUSE_MORPH_TOKENIZER = False\n\ndef plot_confusion_matrix(cm, classes, normalize=False,\n                          title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    return\n\n\n# for Optuna\ndef obj(trial):\n    # C\n    svc_c = trial.suggest_loguniform('C', 1e0, 1e2)\n    # kernel\n    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n    # SVC\n    clf = SVC(C=svc_c, kernel=kernel)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    # 3-fold cross validation\n    score = cross_val_score(clf, X_train, y_train, n_jobs=-1, cv=3)\n    accuracy = score.mean()\n    return 1.0 - accuracy\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description='Train a Support Vector Sentence Classifier')\n    parser.add_argument('csv', help='PATH TO CSVFILE')\n    parser.add_argument('dir', help='TEXTFILE TARGET DIRECTORY')\n    parser.add_argument('model', help='PATH TO PRETRAINED DOC2VEC MODEL FILE')\n    parser.add_argument(\"-N\", \"--n_trial\", dest='n', default=20, type=int,\n                        help='N OF OPTIMIZE TRIALS (Default is 20times)')\n    parser.add_argument(\"-M\", \"--mecab\", dest='mecab', action='store_true',\n                        help='USE MECAB Owakati TAGGER')\n    args = parser.parse_args()\n    PATH_TO_CSVFILE = args.csv\n    TEXTFILE_TARGET_DIR = args.dir\n    PATH_TO_PRETRAINED_DOC2VEC_MODEL = args.model\n    N_OPTIMIZE_TRIAL = args.n\n    USE_MORPH_TOKENIZER = args.mecab\n\n    m = MeCab.Tagger(\"-Owakati\")\n    df = pd.read_csv(PATH_TO_CSVFILE)\n\n    documents = []\n    for fname in tqdm(df.id, desc=\"Reading Files\"):\n        with open(TEXTFILE_TARGET_DIR + fname) as f:\n            if USE_MORPH_TOKENIZER:\n                doc = m.parse(f.read()).strip().split()\n            else:\n                doc = f.read().strip().split()\n        documents.append(doc)\n\n    model = Doc2Vec.load(PATH_TO_PRETRAINED_DOC2VEC_MODEL)\n    document_vectors = [model.infer_vector(s) for s in tqdm(documents)]\n\n    X_train, X_test, y_train, y_test = train_test_split(document_vectors, df.labels,\n                                                        test_size=0.5, random_state=42)\n\n    study = optuna.create_study()\n    study.optimize(obj, n_trials=N_OPTIMIZE_TRIAL)\n    # fits a model with best params\n    clf = SVC(C=study.best_params[\"C\"], kernel=study.best_params[\"kernel\"])\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    # Compute confusion matrix\n    cnf_matrix = confusion_matrix(y_test, y_pred)\n    np.set_printoptions(precision=2)\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=data.categories,\n                          title='Confusion matrix, without normalization')\n    plt.show()\n    # print result\n    print(f\"Acc = {accuracy_score(y_test, y_pred)}\")\n    print(f\"F1 = {f1_score(y_test, y_pred, average='weighted')}\")\n```\n","---\ntitle: DockerでRstanが使えるJupyterLabのサーバーを建てる\ndescription: \"Rstanの環境構築でコケるので。\"\nslug: docker-r-jupyterlab\ndate: 2019-07-22 11:23:52\ncategory: \"Tech Blog\"\ntags: [Docker, R, Jupyter]\nkeyVisual: https://i.gyazo.com/3db0c0080225207a25f263779916a5a4.png\n---\n\nRstanの環境構築でコケるので。\n\n## Dockerfile\n\n```Dockerfile\nFROM jupyter/datascience-notebook\n\nRUN pip install jupyterlab\nRUN jupyter serverextension enable --py jupyterlab\nRUN jupyter labextension install @jupyterlab/git\nRUN pip install jupyterlab-git\nRUN jupyter serverextension enable --py jupyterlab_git\n\nFROM rocker/tidyverse\n\n# Change environment to Japanese(Character and DateTime)\nENV LANG ja_JP.UTF-8\nENV LC_ALL ja_JP.UTF-8\nRUN sed -i '$d' /etc/locale.gen \\\n  && echo \"ja_JP.UTF-8 UTF-8\" >> /etc/locale.gen \\\n  && locale-gen ja_JP.UTF-8 \\\n  && /usr/sbin/update-locale LANG=ja_JP.UTF-8 LANGUAGE=\"ja_JP:ja\"\nRUN /bin/bash -c \"source /etc/default/locale\"\nRUN ln -sf  /usr/share/zoneinfo/Asia/Tokyo /etc/localtime\n\n# Install Japanese fonts\nRUN apt-get update && apt-get install -y \\\n  fonts-ipaexfont\n\n# Install packages\nRUN Rscript -e \"install.packages(c('githubinstall','rstan','ggmcmc','bayesplot','brms'))\"\n\nRUN jupyter lab --no-browser --port=8888\n\n```\n\n## References\n\n- [dockerhub:kazutan/stan-d](https://hub.docker.com/r/kazutan/stan-d/)\n- [dockerhub:jupyter/datascience-notebook](https://hub.docker.com/r/jupyter/datascience-notebook/)\n- [Qiita:DockerでJupyterLabを構築する](https://qiita.com/muk-ai/items/a147cfd2cafc57420b15)\n- [Qiita:Dockerを利用してRStudio ServerでRStan環境を準備する](https://qiita.com/kazutan/items/f1447cbabed8d4dd50b8)\n","---\ntitle: herokuでPython製LINEbot+WebAppをCIする\ndescription: 'LINE Messaging API + line-bot-sdk によるPythonでのチャットボット作成と，同等機能をもつWebアプリケーションを同時に作成し，herokuへデプロイ，継続的インテグレーションでmasterへのpushで自動デプロイさせました。'\nslug: flask-line-bot-heroku\ndate: 2019-07-26 03:14:45\ncategory: 'Tech Blog'\ntags: [Python, flask, チャットボット]\nkeyVisual: https://i.gyazo.com/bfb34348c18382476989e734c3106351.png\n---\n\nLINE Messaging API + line-bot-sdk による Python でのチャットボット作成と，同等機能をもつ Web アプリケーションを同時に作成し，heroku へデプロイ，継続的インテグレーションで master への push で自動デプロイさせました。\n\nコードの詳細等はそもそも[ゴー ☆ ジャスをつくる](https://qiita.com/jg43yr/items/30defcdb69163612fc27)を LINE Bot と Web アプリ化したリポジトリがあるので，そちらを参照すること。\n\n<blockquote class=\"embedly-card\"><h4><a href=\"https://github.com/atsukoba/GorgeousApp\">atsukoba/GorgeousApp</a></h4><p>キミのハートに、レボ☆リューション！. Contribute to atsukoba/GorgeousApp development by creating an account on GitHub.</p></blockquote>\n<script async src=\"//cdn.embedly.com/widgets/platform.js\" charset=\"UTF-8\"></script>\n\n[Web アプリ](https://gorgeous-app.herokuapp.com/)\n\n## heroku の下準備\n\nまず，テキスト処理系のモノを heroku にデプロイするならば`MeCab`+`mecab-python`等の形態素解析器を入れたい。ので，buildpack を複数入れられる`heroku-buildpack-multi`を選択し，python と linuxbrew の buildpack を入れることで MeCab 等の`brew install`を可能にする\n\nまず heroku cli。\n\n```shell\nbrew tap heroku/brew && brew install heroku\nheroku create --buildpack https://github.com/heroku/heroku-buildpack-multi\n```\n\n`.buildpack`へは以下を\n\n```.buildpack\nhttps://github.com/heroku/heroku-buildpack-python.git\nhttps://github.com/sunny4381/heroku-buildpack-linuxbrew.git\n```\n\nlinuxbrew 用の`.celler`に\n\n```.celler\nmecab\nmecab-ipadic\n```\n\nと書いておき，`requirements.txt`も適切にかけば環境が整う。\n\n以下を参照すると良い。(sklearn を動かすために buildpack-multi で conda を入れている)\n\n[heroku で python+django+scikit-learn+mecab(1)](https://qiita.com/kenchin110100/items/6f1c84ac8858525fffc5)\n\n## LINE Developers / Messaging API の設定\n\n![screenshot](https://i.gyazo.com/bfb34348c18382476989e734c3106351.png)\n\n各種アクセストークン等取得しておく。\n以下が多分文字通りわかりやすい。\n\n[LINE BOT の作り方を世界一わかりやすく解説（１）【アカウント準備編】](https://qiita.com/yoshizaki_kkgk/items/bd4277d3943200beab26)\n\n## flask で API を書く\n\nLINE bot 用のテンプレート ([app.py](https://github.com/line/line-bot-sdk-python/blob/master/examples/flask-kitchensink/app.py)) があり，それを使う。そこでは`/callback`への`POST`に対して返答を行うので，それに加えて，最低限ルートへの`GET`, `POST`の処理を書いておく。(`flask.render_template()`でテンプレート HTML を返すようにしておく)\n\nここで，テキスト処理用のモジュールを読んでおいて，`POST`で入力されたテキストに対しての返答を`JSON`で受け取り，`jinja2`で扱えるように`render_template`に渡してあげる。\n\n`linebot.LineBotAPI`と`linebot.WebhookHandler`のインスタンス化に必要な`CHANNEL_ACCESS_TOKEN`と`CHANNEL_SECRET`は環境変数に入れておき，`os.environ.get`で取得する。heroku 上では管理画面から登録し，ローカルでのテストではテキトーに何か入れておく。\n\n`app.run(debug=True)`で API のテストをする。\n\n## Jinja2 を書く\n\nテキスト処理なので，最低限入力ボックスと出力結果の UI はほしい。ので，HTML と CSS を書く。`flask`の app と同階層に`template/`を作成し，そこに html を書いていく。\n\n普通に\n\n```html\n<form action=\"/\" method=\"POST\">\n  <input type=\"text\" name=\"input\" />\n</form>\n```\n\nのように記入すれば`POST`できるし，`Jinja`内では，2 重ブラケット内で\n\n```html\n<div id=\"data\">\n  {{ data[\"key] }}\n</div>\n```\n\n`render_template()`内に渡されたキーワード変数がそのまま辞書としてアクセスできる。\n\nただ，二重ブラケット内で素の Python が書けるわけではない(とくにループやデータのキャストとか)ので，以下等を参照すると良い。\n\n[python のためのテンプレートエンジン「Jinja2」便利な機能](https://qiita.com/kotamatsuoka/items/a95faf6655c0e775ee22)\n\n`templates/`と同階層に`static/`を作成しておけば，そこに css や js を書いてフロントをすこしいじれる。私はここで`sass/`内に sass を書き，`css/style.css`へコンパイルすることで，\n\n```html\n<link rel=\"stylesheet\" href=\"/static/css/style.css\" />\n```\n\nのようにいつもどおり head から読んで使っている。\n\n## heroku と GitHub の連携\n\nheroku で動かすために，`Procfile`を書く。  \nwsgi として`gunicorn`を用いるのが楽で，ポピュラーというか flask だと必須か。\nprocess type は`web`に設定し，gunicorn を起動する。\n\n`web: gunicorn app.app:app --log-file=-`\n\nアプリケーションのインスタンス`flask.Flask`を指定してあげる。  \n`アプリケーションモジュール:アプリケーションインスタンス/関数`という指定方法。\n\n[Gunicorn - Python WSGI HTTP Server for UNIX](https://gunicorn.org/)\n\n![ss](https://i.gyazo.com/75a46844773d4bddcae040fac45bff17.png)\n\nその後は GitHub 上にリモートレポジトリを作成し，heroku のダッシュボード上で認証・連携をする。この時に GUI で`config vars`から LINE から発行されたアクセストークン等を登録できる。\n\nあとは，普通に git push すれば，デプロイが走る。  \nデプロイ中のログに加え Python から`logging`や`print`で出力した内容もダッシュボードのログから確認できるため，そこで逐一チェックすればまあ動くものは作れるはず。\n","---\ntitle: 'iOSプロジェクトへのFirebase導入で詰まったことメモ'\ndescription: 'iOSプロジェクトへのFirebase導入で詰まったことのメモを残します．'\nslug: ios-firebase\ndate: 2020-11-30 00:26:20\ncategory: 'Tech Blog'\ntags: [iOS, Firebase]\nkeyVisual: https://i.gyazo.com/be840f19639279330deb61f636e14685.png\n---\n\n### パッケージマネージャ\n\n[Firebase を iOS プロジェクトに追加する - Google](https://firebase.google.com/docs/ios/setup?hl=ja)を参照すると，現状ではXcode 12.0 以降，CocoaPods 1.9.0 以降が推奨されているので，`Swift Packaging Manager`ではなく`Cocoapods`を使う．\n\n### Property Listの扱い\n\nFirebaseのconsoleでのApp追加時に取得する`plist`を環境毎に分けたい場合．それぞれFirebaseのプロジェクト毎にリネームし，iOSプロジェクト内に配置．\n\n```swift\n// AppDelegate.swift\n\n// MARK: - Firebase init\nlet configFileName: String\n#if DEBUG\nconfigFileName = \"GoogleService-dev-Info\"\n#else\nconfigFileName = \"GoogleService-prod-Info\"\n#endif\nguard let filePath = Bundle.main.path(forResource: configFileName, ofType: \"plist\"),\n    let options = FirebaseOptions(contentsOfFile: filePath) else {\n    fatalError(\"Firebase plist file is not found.\")\n}\nFirebaseApp.configure(options: options)\n```\n\n### Crashlystics導入\n\nBuild PhaseでビルドしたApp内に`plist`を配置する必要がある．\n\n[Get started with Firebase Crashlytics - Google](https://firebase.google.com/docs/crashlytics/get-started)\n\nビルド時の最後に走らせる必要があるので，`Podfile`に定義する．\n\n```Ruby\n# Podfile\n\ntarget 'PROJECT_NAME' do\n  # Comment the next line if you don't want to use dynamic frameworks\n  use_frameworks!\n  \n  pod 'Firebase/Crashlytics'\n  script_phase :name=> 'FirebaseCrashlytics',\n                 :script=> '\"${PODS_ROOT}/FirebaseCrashlytics/run\"',\n                 :input_files=> ['$(SRCROOT)/$(BUILT_PRODUCTS_DIR)/$(INFOPLIST_PATH)']\n```\n\nそしてその前に各環境毎の`plist`をApp内の指定したパスにコピーしておく．ｌｐれはXCodeのCodegen上にスクリプトを足しておく．\n\n```shell\n# Type a script or drag a script file from your workspace to insert its path.\n\n# Crashlytics用にGoogleService-Info.plistをビルドディレクトリにコピーする必要があるた\nPATH_TO_GOOGLE_PLISTS=\"${PROJECT_DIR}/bengoshi-ios/Firebase\"\n\ncase \"${CONFIGURATION}\" in\n\"Debug\" )\ncp -r \"$PATH_TO_GOOGLE_PLISTS/GoogleService-dev-Info.plist\" \"${BUILT_PRODUCTS_DIR}/${PRODUCT_NAME}.app/GoogleService-Info.plist\" ;;\n\n\"Release\" )\ncp -r \"$PATH_TO_GOOGLE_PLISTS/GoogleService-prod-Info.plist\" \"${BUILT_PRODUCTS_DIR}/${PRODUCT_NAME}.app/GoogleService-Info.plist\" ;;\n\n*)\n;;\nesac\n```\n\nFirebase App Distributionの場合はアーカイブ時に `Rebuild from Bitcode`をオフにする．これによりdSYMが同梱されるためアップロードの必要がなくなるらしい．\nApp Store Connectにアップロードする際は，\n\n## Firebase Analytics\n\n以下のように各種イベントの種類等定義しておく\n\n```swift\nimport Foundation\nimport Firebase\nimport FirebaseAnalytics\n\nclass EventLogs {\n    // MARK: - LogIn\n    static func loginInputTelNumber(success: Bool, error_type: String? = nil) {\n        if success {\n            Analytics.logEvent(\"ios_login_input_tel_number_succeed\",\n                               parameters: nil)\n        } else {\n            Analytics.logEvent(\"ios_login_input_tel_number_failure\",\n                               parameters: [\"error_type\": error_type as Any])\n        }\n    }\n    static func login(success: Bool, error_type: String? = nil) {\n        if success {\n            Analytics.logEvent(\"login\", parameters: [\"method\": \"iOS\"])\n            Analytics.logEvent(\"ios_login_succeed\", parameters: nil)\n        } else {\n            Analytics.logEvent(\"ios_login_failure\",\n                               parameters: [\"error_type\": error_type as Any])\n        }\n    }\n    static func logout() {\n        Analytics.logEvent(\"ios_logout\", parameters: nil)\n    }\n    // MARK: - UI\n    static func openModal() {\n        Analytics.logEvent(\"ios_ui_modal_open\", parameters: nil)\n    }\n    static func closeModal() {\n        Analytics.logEvent(\"ios_ui_modal_close\", parameters: nil)\n    }\n: \n:\n```\n\nただ，Firabse AnaltyicsのEventは反映されるのに時間がかかるので，DebugView 機能を用いる．Xcodeで、`Product -> Scheme -> Edit scheme`のArgumentsタブで `-FIRDebugEnabled`を引数に設定する．すると，数十秒のラグでデバッグしながらイベントを確認できる．\n\n### 参考\n\n<https://www.apps-gcp.com/introduction-of-firebase-analytics/#i-5>\n","---\ntitle: JupyterLabのExtensionメモ\ndescription: 'Jupyter Labの拡張機能メモ'\nslug: jlab-ext\ndate: 2019-04-19 05:07:47\ncategory: 'Tech Blog'\ntags: [Jupyter]\nkeyVisual: https://i.gyazo.com/6d52df1f28b5f067804814ac8216f645.png\n---\n\nJupyter Lab の拡張機能メモ\n\n[Extensions — JupyterLab 3.0.6 documentation](https://jupyterlab.readthedocs.io/en/stable/user/extensions.html)\n\n## Git\n\n```bash\njupyter labextension install @jupyterlab/git\npip install -e git+https://github.com/SwissDataScienceCenter/jupyterlab-git.git@fix-git-current-dir#egg=jupyterlab_git\njupyter serverextension enable --py jupyterlab_git\n```\n\n## Var Inspector\n\n```bash\njupyter labextension install @lckr/jupyterlab_variableinspector\n```\n\n## ToC\n\n```bash\njupyter labextension install @jupyterlab/toc\npip install jupyterlab_code_formatter\njupyter labextension install @ryantam626/jupyterlab_code_formatter\njupyter serverextension enable --py jupyterlab_code_formatter\n```\n\n## autoPEP8\n\n```bash\npip install autopep8\n\n```\n","---\ntitle: 任天堂SwitchのジョイコンをPython経由で楽器にする\ndescription: \"Bluetooth接続したJoy-Conからhid経由で情報取得するPythonライブラリ`joycon-python`の開発に参加し，その`joycon-python`を用いて信号をOSC (Open Sound Control)に飛ばすスクリプト`joycon-osc`を作成し，その`joycon-osc`を用いて送信した情報をMaxで受け取って音にしました。\"\nslug: joycon-driver\ndate: 2020-01-26 03:40:48\ncategory: \"Tech Blog\"\ntags: [Python, Max/MSP, Bluetooth, Open Sound Control]\nkeyVisual: https://i.gyazo.com/4835972f8fde2c7671125a408d52ffa5.jpg\n---\n\nBluetooth接続したJoy-Conからhid経由で情報取得するPythonライブラリ`joycon-python`の開発に参加し，その`joycon-python`を用いて信号をOSC (Open Sound Control)に飛ばすスクリプト`joycon-osc`を作成し，その`joycon-osc`を用いて送信した情報をMaxで受け取って音にしました。\n\nこんなものです。\n\n<div style=\"margin:0 auto;width:fit-content;\">\n<blockquote class=\"twitter-tweet\" data-lang=\"en\" data-dnt=\"true\" data-theme=\"dark\"><p lang=\"ja\" dir=\"ltr\"><a href=\"https://t.co/hjYsuvJFMd\">pic.twitter.com/hjYsuvJFMd</a></p>&mdash; atsuya kobayashi (@atsuyakoba) <a href=\"https://twitter.com/atsuyakoba/status/1221912455019782144?ref_src=twsrc%5Etfw\">January 27, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n</div>\n\n## joy-con to Python\n\nこう言うと人に驚かれるのだが，僕は任天堂Switchを持っていないのに任天堂Switchのコントローラーであるジョイコンを持っています。なぜならジョイコンだけ買った(買わされた)から。\n\nということで，ジョイコンをmacにつなげて何か遊べないかと考えていたときに，[Qiita: Joy-ConにPythonからBluetooth接続をして6軸センサーと入力情報を取得する](https://qiita.com/tokoroten-lab/items/9a5d81c8f640ecaff7a9#comment-a6875db57d685403e37f)という記事を発見。すぐさま著者が公開していたリポジトリを訪問し，forkし，パッケージ化し，PRを出した（らめっちゃ丁寧なコードレビューをしてもらい感動した）。それが以下のリポジトリ。\n\n<blockquote class=\"embedly-card\"><h4><a href=\"https://github.com/tokoroten-lab/joycon-python\">tokoroten-lab/joycon-python</a></h4><p>driver for Joy-Con (Nintendo Switch). Contribute to tokoroten-lab/joycon-python development by creating an account on GitHub.</p></blockquote>\n<script async src=\"//cdn.embedly.com/widgets/platform.js\" charset=\"UTF-8\"></script>\n\n今ではPyPiへの公開もされており，`pip install joycon-python`で使える。\n\n## joy-con to osc\n\n次に，ジョイコンの状態を監視してOSCを送るスクリプトを作成した。それも以下のリポジトリとして公開している。\n\n<blockquote class=\"embedly-card\"><h4><a href=\"https://github.com/atsukoba/joycon-osc\">atsukoba/joycon-osc</a></h4><p>Send OSC (Open Sound Control) by Joy-Con (Nintendo Switch). Contribute to atsukoba/joycon-osc development by creating an account on GitHub.</p></blockquote>\n\n## joy-con to Max/MSP\n\nOSCを飛ばす準備をしたら，あとはMax上で受け取って音にする。\n\n<div style=\"margin:0 auto;width:fit-content;\">\n<blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-theme=\"dark\"><p lang=\"und\" dir=\"ltr\"><a href=\"https://t.co/iW8arvdzSw\">pic.twitter.com/iW8arvdzSw</a></p>&mdash; atsuya kobayashi (@atsuyakoba) <a href=\"https://twitter.com/atsuyakoba/status/1220526326647386112?ref_src=twsrc%5Etfw\">January 24, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n</div>\n","---\ndescription: \"\"\ntitle: \"Jupyter NotebookのCustom CSS\"\nslug: jupyterstyle\ndate: 2019-04-24 10:33:59\ncategory: \"Tech Blog\"\ntags: [CSS, Jupyter]\nkeyVisual: https://i.gyazo.com/5567d5a7d74c2989ba069c3563b6061a.png\n---\n\nJupyter Notebookでは`~/.jupyter/custom/custom.css`にCSSを記述すれば自由にスタイルを変えられる。テーマを変更するのではなく，自前でCSSを書く。  \n例えば[あなたの生産性を向上させるJupyter notebook Tips](https://recruit-tech.co.jp/blog/2018/10/16/jupyter_notebook_tips/#b24)では，シンタックスハイライトまで変更している。\n\n<!-- more -->\n\n例えば，以下のCSSを当てると\n\n<script src=\"https://gist.github.com/atsukoba/beb7ec3fd1927dacf2f6df4b0f209f22.js\"></script>\n\n### Before\n\n![before style](https://i.gyazo.com/975ecbdd77872c8792108ed679284c91.png)\n\n### After\n\n![after style](https://i.gyazo.com/5567d5a7d74c2989ba069c3563b6061a.png)\n\nきゃー便利。`animate.css`とかで無駄なパララックスを実装して遊んでも良さそうだ。ただJupyter Labはマークアップの構造がそもそもかなり違うため，流用はでき無さそう。\n\n","---\ntitle: 'Wikipediaから検索するLINE botを作った'\ndescription: '`flask`, `line-bot-sdk` と `wikipedia`で作って，`heroku`にデプロイします。'\nslug: line-wikipedia\ndate: 2019-05-04 00:26:20\ncategory: 'Tech Blog'\ntags: [Python, チャットボット]\nkeyVisual: https://repository-images.githubusercontent.com/184452650/ee9d3b80-6cab-11e9-9261-01e61f18fa3c\n---\n\n`flask`, `line-bot-sdk` と `wikipedia`で作って，`heroku`にデプロイします。\n\n<a href=\"http://nav.cx/9xRAMW8\" >ぜひ使って下さい。</a><a href=\"http://nav.cx/9xRAMW8\" style=\"width: 120px;height: 35px;display: inline-block;border-radius: 10px;background-image: url(https://scdn.line-apps.com/n/line_add_friends/btn/ja.png);background-size: cover;\">\n</a>\n\n\n## 完成図\n\n![screenshot](https://repository-images.githubusercontent.com/184452650/ee9d3b80-6cab-11e9-9261-01e61f18fa3c)\n\nレポジトリは[こちら](https://github.com/atsukoba/Wikipedia-LINEbot)\n\n---\n\n## 準備\n\n`pip install -r requirements.txt`させたいので，上記３つを requirements.txt へ記述し，heroku の設定と LINE@の設定を終わらせておく (アクセストークンとチャンネルシークレットあたりを入手しておく)。`runtime.txt`と`procfile`を置く。基本[Heroku で LINE BOT(python)を動かしてみた](https://qiita.com/akabei/items/38f974716f194afea4a5)を真似た。\n\n---\n\n## Wikipedia について\n\nみんなだいすき Wikipedia。\n\n```bash\npip instal wikipedia\n```\n\nで入るライブラリである[Wikipedia](https://pypi.org/project/wikipedia/)は非常に便利で，Python 用の[Media Wiki API](https://ja.wikipedia.org/w/api.php)のラッパーだと解釈している。`requests`で API 叩いて，`BeautifulSoup4`か何かでマークアップをバラして，返してくれるものだったはず。\n\n```python\nimport wikipedia\n\nwikipedia.set_lang(\"ja\")\n```\n\nで日本語 Wikipedia に設定した後，\n\n```python\nwikipedia.search(\"文字列\")\n```\n\nをすることで各`ページ名`のリストが返り，書く Wikipedia のページはその名前(タイトル)が ID となっており，\n\n```python\nwikipedia.page(\"ページ名\")\n```\n\nで`wikipedia.WikipediaPage`オブジェクトを取得できる。この page オブジェクトが`categories`, `links`, `content`, `summary`などの attributes をもっており，これらは基本的に URL か文字列かのリストである。\n\n```python\n>>> help(wikipedia.WikipediaPage)\n>>>\n\"\"\"\ncategories\n    List of categories of a page.\ncontent\n    Plain text content of the page, excluding images, tables, and other data.\ncoordinates\n    Tuple of Decimals in the form of (lat, lon) or None\nimages\n    List of URLs of images on the page.\nlinks\n    List of titles of Wikipedia page links on a page.\n    Only includes articles from namespace 0, meaning no Category, User talk, or other meta-Wikipedia pages.\nparent_id\n    Revision ID of the parent version of the current revision of this\n    page. See ``revision_id`` for more information.\nreferences\n    List of URLs of external links on a page.\n    May include external links within page that aren't technically cited anywhere.\nrevision_id\n    Revision ID of the page.\n    The revision ID is a number that uniquely identifies the current\n\"\"\"\n```\n\n今回つくる LINE Bot では，検索単語に対して取得した候補の中から 1 ページ選び，そのページの summary (タイトル直後の概要・OGP とかに表示される?)とページへのリンクを LINE トークルームにかえしてあげよう，というものを作ることにした。\n\n---\n\n## ファイルとか\n\n```\n.\n├── Procfile\n├── README.md\n├── __pycache__\n│   ├── app.cpython-36.pyc\n│   └── parser.cpython-36.pyc\n├── app.py\n├── assets\n│   └── img\n│       └── linebot-icon.png\n├── messenger.py\n├── parser.py\n├── requirements.txt\n├── runtime.txt\n└── test.py\n```\n\n---\n\n## `app.py`\n\n次に，`flask`ベースでアプリケーション部分をササっと書きますが，これもほぼコピペ。面倒な部分を`linebot`が隠してくれていて，非常に便利。\n\n```python\nfrom flask import Flask, request, abort\n\nfrom linebot import (\n    LineBotApi, WebhookHandler\n)\nfrom linebot.exceptions import (\n    InvalidSignatureError\n)\nfrom linebot.models import (\n    MessageEvent, TextMessage, TextSendMessage,\n)\n\nimport parser\nimport os\n\napp = Flask(__name__)\n\nYOUR_CHANNEL_ACCESS_TOKEN = os.environ.get(\"YOUR_CHANNEL_ACCESS_TOKEN\")\nYOUR_CHANNEL_SECRET = os.environ.get(\"YOUR_CHANNEL_SECRET\")\n\nline_bot_api = LineBotApi(YOUR_CHANNEL_ACCESS_TOKEN)\nhandler = WebhookHandler(YOUR_CHANNEL_SECRET)\n\n\n@app.route(\"/callback\", methods=['POST'])\ndef callback():\n    # get X-Line-Signature header value\n    signature = request.headers['X-Line-Signature']\n\n    # get request body as text\n    body = request.get_data(as_text=True)\n    app.logger.info(\"Request body: \" + body)\n\n    # handle webhook body\n    try:\n        handler.handle(body, signature)\n    except InvalidSignatureError:\n        print(\"Invalid signature. Please check your channel access token/channel secret.\")\n        abort(400)\n\n    return 'OK'\n\n\n@handler.add(MessageEvent, message=TextMessage)\ndef handle_message(event):\n    line_bot_api.reply_message(\n        event.reply_token,\n        TextSendMessage(text=parser.answer(event.message.text)))\n\n\nif __name__ == \"__main__\":\n    port = int(os.getenv(\"PORT\", 5000))\n    app.run(host=\"0.0.0.0\", port=port)\n```\n\nline-bot-sdk-python の公式リポジトリに，`app.py`として flask でのサンプルが公開されている。\n[[sample] app.py](https://github.com/line/line-bot-sdk-python/blob/master/examples/flask-echo/app.py)\n\nまた，今回はそもそも README にガッツリ載っていたものを使った。[sample code on GitHub](https://github.com/line/line-bot-sdk-python/blob/master/README.rst)\n\n---\n\n### `parser.py`\n\nparser はモジュールの変数として言語設定を持っている。設計として微妙ですかね？モジュール(グローバル)変数は。  \nなんか使い方間違えたりとか，ヘルプ出したい時のための`usage()`は未実装。  \n意外と`WikipediaPage.summary`の文字数が長く，Messaging API の上限を叩いてしまったときのために，1500 文字以上は切っている。\n\n```python\nimport wikipedia\n\n\n# init language setting\nlang = \"ja\"\nwikipedia.set_lang(lang)\n\ndef init() -> None:\n    global lang\n    wikipedia.set_lang(lang)\n\n\ndef tokenize(text: str) -> list:\n    \"\"\"Tokenize input Sentence to list of word\"\"\"\n    splited = text.split()\n    if len(splited) == 1:\n        return splited\n    elif len(splited) == 2:\n        if splited[0] in wikipedia.languages.fn().keys():\n            change_lang(splited[0])\n        return splited[1]\n    else:\n        usage()\n\ndef search(text: str, rank=0) -> \"wikipedia.wikipedia.WikipediaPage\":\n    \"\"\"Search Wikipedia page by Word\n    arg\n    ---\n    rank : int : Return the contents of the search result of the set rank.\n    \"\"\"\n    try:\n        page = wikipedia.page(wikipedia.search(text)[rank])\n    except wikipedia.exceptions.DisambiguationError:\n        page = wikipedia.page(wikipedia.search(text)[rank+1])\n    return page\n\n\ndef encode(page: \"wikipedia.wikipedia.WikipediaPage\", threshold=1500) -> str:\n    \"\"\"Transform data into the text for LINE message\n    \"\"\"\n    summary = page.summary\n    if len(summary) > threshold:\n        summary = summary[:threshold] + \"...\"\n\n    return f\"Result: {page.title}\\n\\n{summary}\\n\\n{page.url}\"\n\n\ndef answer(text: str) -> str:\n    init()\n    word = tokenize(text)\n    page = search(word)\n    return encode(page)\n\n\ndef change_lang(language: str) -> None:\n    wikipedia.set_lang(language)\n    return\n\ndef usage():\n    pass\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.parse_args()\n```\n\n一応ちゃんと**PEP8**スタイルだし，type hints も docstring 書いている。クセにしとおきたい。\n\n---\n\n### デプロイ等\n\n```bash\n$ heroku login\n$ heroku create heroku-line-bot\n$ heroku config:set LINE_CHANNEL_SECRET=\"<Channel Secret>\"\n$ heroku config:set LINE_CHANNEL_ACCESS_TOKEN=\"<アクセストークン>\"\n$ git push heroku master\n```\n\n---\n\n### references\n\n- [LINE Messaging API SDK for Python - GitHub](https://github.com/line/line-bot-sdk-python)\n- [Python で Line bot を作ってみた - Qiita](https://qiita.com/kro/items/67f7510b36945eb9689b)\n- [Messaging API SDK - LINE Developers](https://developers.line.biz/ja/docs/messaging-api/line-bot-sdk/)\n- [Heroku でサンプルボットを作成する - LINE Developers](https://developers.line.biz/ja/docs/messaging-api/building-sample-bot-with-heroku/)\n- [heroku に Flask アプリをデプロイする - Qiita](https://qiita.com/msrks/items/c57e0168fb89f160d488)\n","---\ntitle: 'MediaPipeによるお手軽手認識とリアルタイムおもちゃのためのOSC連携'\ndescription: ''\nslug: media-pipe-osc\ndate: 2020-12-22 00:26:20\ncategory: 'Tech Blog'\ntags: [MediaPipe, Open Sound Control]\nkeyVisual: https://i.gyazo.com/229d4f1e990ac46f3d4e4b5dfd9806c3.jpg\n---\n\nMediapipe: <https://github.com/google/mediapipe>\n\nMediaPipe Hand: <https://google.github.io/mediapipe/solutions/hands>\n\n---\n\nMediaPipeはBazarevskyらがCVPR2019で発表したオープンソースの機械学習用フレームワークで，そこで用いられている手認識機能は，single-shot手のひら認識アルゴリズムとlandmark認識モデルが組み合わされたものです（[Google AI Blog: On-Device, Real-Time Hand Tracking with MediaPipe](https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html)）\n\n[![Image from Gyazo](https://i.gyazo.com/bb83a5b6ca5f6ca7865836d9f1e9e3d7.jpg)](https://gyazo.com/bb83a5b6ca5f6ca7865836d9f1e9e3d7)\n\n手の形状は以下の各ランドマークの座標として取得できます．毎フレーム推論が走り，cv座標での値が取得できます．\n\n[![Image from Gyazo](https://i.gyazo.com/d4a41006b4110401a6fb593da4c4d544.png)](https://gyazo.com/d4a41006b4110401a6fb593da4c4d544)\n\n返ってくる`landmark`のオブジェクトは以下のようにして座標の`float`が取り出せます\n\n```python\nfor hand_idx, landmarks in enumerate(multi_hand_landmarks):\n    for point_idx, points in enumerate(landmarks.landmark):\n        print(f\"Hand: {hand_idx}, {HAND_LANDMARK_NAMES[point_idx]},\"\n                      + f\"x:{points.x} y:{points.y} z:{points.z}\")\n```\n\nこの時の`HAND_LANDMARK_NAMES`は，以下のような順番になっています．\n\n```python\nHAND_LANDMARK_NAMES = [\n    \"wrist\",\n    \"thumb_1\",\n    \"thumb_2\",\n    \"thumb_3\",\n    \"thumb_4\",\n    \"index_1\",\n    \"index_2\",\n    \"index_3\",\n    \"index_4\",\n    \"middle_1\",\n    \"middle_2\",\n    \"middle_3\",\n    \"middle_4\",\n    \"ring_1\",\n    \"ring_2\",\n    \"ring_3\",\n    \"ring_4\",\n    \"pinky_1\",\n    \"pinky_2\",\n    \"pinky_3\",\n    \"pinky_4\"\n]\n```\n\n今回はこのMediaPipeによるリアルタイム手認識を用いて何かしらのインタラクティブ作品やWekinator等を用いるジェスチャ認識などのためのOpen Sound Controlでのデータ送信をプロトタイプします．\n\n## スクリプト\n\n```python\n# Atsuya Kobayashi 2020-12-22\n# Reference: https://google.github.io/mediapipe/solutions/hands\n# LICENCE: MIT\n\nfrom itertools import chain\n\nimport mediapipe as mp\nfrom cv2 import cv2\nfrom pythonosc import udp_client\n\nIP = \"127.0.0.1\"\nPORT = 7474\nVIDEO_DEVICE_ID = 0\nRELATIVE_AXIS_MODE = True\n\nHAND_LANDMARK_NAMES = [\n    \"wrist\",\n    \"thumb_1\",\n    \"thumb_2\",\n    \"thumb_3\",\n    \"thumb_4\",\n    \"index_1\",\n    \"index_2\",\n    \"index_3\",\n    \"index_4\",\n    \"middle_1\",\n    \"middle_2\",\n    \"middle_3\",\n    \"middle_4\",\n    \"ring_1\",\n    \"ring_2\",\n    \"ring_3\",\n    \"ring_4\",\n    \"pinky_1\",\n    \"pinky_2\",\n    \"pinky_3\",\n    \"pinky_4\"\n]\n\n\ndef extract_detected_hands_points(multi_hand_landmarks,\n                                  send_osc_client=None):\n\n    if multi_hand_landmarks is not None:\n        for hand_idx, landmarks in enumerate(multi_hand_landmarks):\n            for point_idx, points in enumerate(landmarks.landmark):\n\n                # if you want to check data on console\n                print(f\"Hand: {hand_idx}, {HAND_LANDMARK_NAMES[point_idx]},\"\n                      + f\"x:{points.x} y:{points.y} z:{points.z}\")\n                \"\"\"\n                if you want to send data to addresses correspoding\n                to landmarks names on detected hands, use berow\n                \"\"\"\n                # if send_osc_client is not None:\n                #     send_osc_client.send_message(f\"/{HAND_LANDMARK_NAMES[point_idx]}\",\n                #                                  [points.x, points.y])\n\n            \"\"\"if you want to send data to single input address, use berow\"\"\"\n            if send_osc_client is not None:\n                send_osc_client.send_message(\n                    f\"/YOUR_OSC_ADDRESS\",\n                    list(chain.from_iterable([[p.x, p.y] for p in landmarks.landmark])))\n\n\nif __name__ == \"__main__\":\n\n    mp_drawing = mp.solutions.drawing_utils\n    mp_hands = mp.solutions.hands\n\n    hands = mp_hands.Hands(\n        min_detection_confidence=0.5, min_tracking_confidence=0.5)\n\n    cap = cv2.VideoCapture(VIDEO_DEVICE_ID)\n\n    osc_client = udp_client.SimpleUDPClient(IP, PORT)\n\n    while cap.isOpened():\n        success, image = cap.read()\n        if not success:\n            print(\"Ignoring empty camera frame.\")\n            # If loading a video, use 'break' instead of 'continue'.\n            continue\n\n        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n        image.flags.writeable = False\n        results = hands.process(image)\n        extract_detected_hands_points(results.multi_hand_landmarks,\n                                      send_osc_client=osc_client)\n        image.flags.writeable = True\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                mp_drawing.draw_landmarks(\n                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n        cv2.imshow('Detected Hands', image)\n\n        if cv2.waitKey(5) & 0xFF == 27:\n            break\n\n    hands.close()\n    cap.release()\n```\n","---\ntitle: '複数の.ipynb をまとめて目次生成'\ndescription: '自身のGitHub上で自身の学習記録やボイラープレートをカンペ集として残している([リポジトリ](https://github.com/atsukoba/cheatbooks))のですが，同じディレクトリ直下に複数のノートブックを保存しています。で，その各ノートブック内のMarkdownに記述されている部分を抽出するスクリプトを書きました。'\nslug: nb-table-generator\ndate: 2019-02-16 23:01:27\ncategory: 'Tech Blog'\ntags: [Python, Jupyter]\nkeyVisual: https://i.gyazo.com/8200e850ac205df84fd106d9c6d82e38.png\n---\n\n自身の GitHub 上で自身の学習記録やボイラープレートをカンペ集として残している([リポジトリ](https://github.com/atsukoba/cheatbooks))のですが，同じディレクトリ直下に複数のノートブックを保存しています。で，その各ノートブック内の Markdown に記述されている部分を抽出するスクリプトを書きました。\n\n### Usage\n\n```shell\n% python nb_table_generator.py\n```\n\nor\n\n```python\nimport nb_table_generator\n\nnb_table_generator.main(file_name=\"output_file_name\")\n```\n\n### src\n\n<script src=\"https://gist.github.com/atsukoba/b284847b58c5d4598580dfe9539669e2.js\"></script>\n","---\ntitle: 'nodejsでローカルにoscを送るwebアプリケーションを作る (SocketIO, Express)'\ndescription: '誰でもoscを通して作品に参加できるようなwebアプリケーションをプロトタイピングしました。'\nslug: osc-webapp\ndate: 2019-10-08 17:50:42\ncategory: 'Tech Blog'\ntags: [Open Sound Control, nodejs, express]\nkeyVisual: https://i.gyazo.com/0ec1a0958d7e3a96e45e0503ca9497ad.png\n---\n\n誰でも osc を通して作品に参加できるような web アプリケーションをプロトタイピングしました。[リポジトリはこちら](\"https://github.com/atsukoba/osc-webapp\")。\n\n## OSC (Open Sound Control)\n\nOSC とは: [opensoundcontrol.org](http://opensoundcontrol.org/) や[wikipedia](https://ja.wikipedia.org/wiki/OpenSound_Control)を参照。\n\nyoppa.org の[openFramewoks – OSC (Open Sound Control) を利用したネットワーク連携](https://yoppa.org/ma2_10/2279.html)が非常にためになる。\n\nUDP 上で MIDI みたいなものを Max/MSP に送ったりできるので非常に便利。今回はこいつを LAN 上だけならずインターネット上のスマートフォンからローカルに受け付ける，展示をよりインタラクティブにするためのツールのプロトタイピングを行う。\n\n(以下リポジトリを参照していただければローカルでアプリケーション動かせます）\n\n<blockquote class=\"embedly-card\"><h4><a href=\"https://github.com/atsukoba/osc-webapp\">atsukoba/osc-webapp</a></h4><p>Serve OSC from www. Contribute to atsukoba/osc-webapp development by creating an account on GitHub.</p></blockquote>\n<script async src=\"//cdn.embedly.com/widgets/platform.js\" charset=\"UTF-8\"></script>\n\n## 概観\n\nnodejs/express でサーバを立てて，軽くて双方向に使える websocket でユーザのアプリケーション上での動きを捉える。localhost に立てたサーバを ngrok で公開し，その URL(とローカルの IP アドレス)を QR コードに出力するところまでを実装する。\n\n軽くフロントを書いてデモをつくり，簡単なボタンとそのボタンに対応したメッセージをローカルの osc に送れるかを確認する。\n\n![alt](oscweb_screenshots.png 'デモのスクリーンショット')\n\n### express\n\n```shell\nnpm install express-generator -g\nexpress --view=ejs osc-webapp\ncd osc-webapp\nnpm install\n```\n\n`socket.io`を使用する。\n\n```javascript\nconst app = express()\nconst http = require('http').Server(app)\nconst io = require('socket.io')(http)\n```\n\n### osc\n\n`node-osc`([npm: node-osc](https://www.npmjs.com/package/node-osc))を利用する。利用例としては以下のような感じ\n\n```javascript\nconst osc_portnum = 5050\nconst client = new osc.Client('127.0.0.1', osc_portnum)\n\nio.of('osc').on('connection', (socket) => {\n  socket.on('message', (obj) => {\n    console.log('osc: ' + obj)\n    obj = JSON.parse(obj)\n    let sendObj = new osc.Message(obj.address)\n    sendObj.append(obj.args)\n    client.send(sendObj)\n    let dt = new Date()\n    io.of('osc').send(\n      `${dt.toFormat('HH24:MI:SS')} : osc message received: ${obj.args}`,\n    )\n  })\n})\n```\n\n### qrcode\n\n`os`, `qrcode`([npm: qrcode](https://www.npmjs.com/package/qrcode)) を用いて，ローカル IP アドレスと`ngrok`([npm: ngrok](https://www.npmjs.com/package/ngrok))で生成した URL を QR にする。\n\n```javascript\nconst ngrok = require('ngrok')\nconst qrcode = require('qrcode')\n\n// get local ip addresses\nlet interfaces = os.networkInterfaces()\nlet addresses = []\nfor (let k in interfaces) {\n  for (let k2 in interfaces[k]) {\n    let address = interfaces[k][k2]\n    if (address.family === 'IPv4' && !address.internal) {\n      addresses.push(address.address)\n    }\n  }\n}\nconsole.log(`local ip addresses: ${addresses}`)\nconsole.log(`FOR LOCAL NEWORK PARTICIPANTS`)\nqrcode.toString(\n  `http://${addresses[0]}:${portnum}`,\n  { type: 'terminal' },\n  (err, str) => {\n    console.log(str)\n  },\n)\n\n// make ngrok tunnel\nconsole.log(`FOR WWW PARTICIPANTS`)\n;(async () => {\n  let url = await ngrok.connect(portnum)\n  console.log('ngrok URL: ' + url)\n  qrcode.toString(url, { type: 'terminal' }, (err, str) => {\n    console.log(str)\n  })\n})()\n```\n\n### つかう\n\n```shell\nnpm start\n```\n\nこれで QR コードが生成されるので，それをシェアすれば osc を送れる。同一 LAN にいるなら，上部の QR コードでおｋ。config ファイルでポート番号を指定し，デモはクライアント側の main.js で(今は)送る osc メッセージをハードコードしているので，適宜それを編集して使っていただければと思う。\n\n![gif](https://i.gyazo.com/3872867c437f9bb2db573f1f3f2b69d1.gif)\n\n### reference\n\n- [Qiita: node.js と Processing を OSC でやりとり](https://qiita.com/tkyko13/items/d219a509d8367e272055)\n- [yoppa.org: Processing Libraries 3 : oscP5 – OSC によるアプリケーション間通信](https://yoppa.org/sfc_design16/7927.html)\n","---\ntitle: 'Processingのvscode開発環境を構築(Mac)'\ndescription: 'Processingのvscode開発環境をMac向けに構築しました'\nslug: processing-on-vscode\ndate: 2019-08-25 07:16:23\ncategory: 'Tech Blog'\ntags: [Processing, Visual Studio Code]\nkeyVisual: https://i.gyazo.com/2458c026d0085fae794cd1f5ff8b5bfd.jpg\n---\n\nProcessing の IDE が非常にイヤなので，まずは[Qiita: Processing を Visual Studio Code で動かしたい](https://qiita.com/jacynthe/items/d31eaa77496295c10556)を参照し設定する。しかしこの記事のやり方だと，1 スケッチ毎に`.vscode`による設定をしなければならずイヤなので，例えば[Processing でゼロから学ぶプログラミング・ビジュアルアートの公式リポジトリ](https://github.com/cocopon/zero-pde)なんかを clone してきて試す時に，プロジェクトフォルダに内包されている複数のスケッチを即時実行できるように改良したメモです。\n\n## 準備\n\nまずは上述の[Qiita 記事](https://qiita.com/jacynthe/items/d31eaa77496295c10556)通りにセッティングを行う。そしたら Processing のインストール。ラクなので`Homebrew`を使う。\n\n### install.sh\n\n```shell\n# homebrew\nif !(type \"brew\" > /dev/null 2>&1); then\n    echo \"install HomeBrew...\"\n    ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" < /dev/null 2> /dev/null ; brew install caskroom/cask/brew-cask 2> /dev/null\n\nelse\n    echo \"Homebrew is already installed\"\nfi\n\nif !(type \"processing-java\" > /dev/null 2>&1); then\n    brew tap caskroom/cask\n    brew install brew-cask\n    echo \"installing Processing via Homebrew...\"\n    brew cask install -v processing\n\n    echo \"Open Processing and install processing-java (from menu bar > Tools > install processing-java)\"\n    echo \"input path to Processing.app: \"\n    read pjpath\n    echo \"adding path\"\n    sudo ln -s ${pjpath}/processing-java /usr/local/bin/\nelse\n    pjpath=$(which processing-java)\n    echo \"processing-java is already installed : ${pjpath}\"\nfi\n\necho \"add path to vscode setting (tasks.json)\"\nsed -i -e \"s|\\\"command\\\":.*|\\\"command\\\": \\\"${pjpath}\\\",|g\" .vscode/tasks.json\n```\n\nそして，生成・編集された`tasks.json`の`\"args\"`の`\"--sketch\"`部分を，[visualstudio.com: Variables Reference](https://code.visualstudio.com/docs/editor/variables-reference)を頼りに編集する。\n\n### .vscode/tasks.json\n\n```json\n\"--sketch=${workspaceRoot}/${relativeFileDirname}\"\n```\n\nこれによりスケッチフォルダのパスがちゃんと渡される。\n\n### Variable Reference\n\n```txt\n${workspaceFolder} - the path of the folder opened in VS Code\n${workspaceFolderBasename} - the name of the folder opened in VS Code without any slashes (/)\n${file} - the current opened file\n${relativeFile} - the current opened file relative to workspaceFolder\n${relativeFileDirname} - the current opened file's dirname relative to workspaceFolder\n${fileBasename} - the current opened file's basename\n${fileBasenameNoExtension} - the current opened file's basename with no file extension\n${fileDirname} - the current opened file's dirname\n${fileExtname} - the current opened file's extension\n${cwd} - the task runner's current working directory on startup\n${lineNumber} - the current selected line number in the active file\n${selectedText} - the current selected text in the active file\n${execPath} - the path to the running VS Code executable\n```\n\nこれにより，プロジェクトフォルダ直下の`.pde`でなくても，`Command + Shift + B`で実行できる。ウレシイ。\n\nここで，試しに`git submodule add git@github.com:cocopon/zero-pde.git`をして，Processing でゼロから学ぶプログラミング・ビジュアルアートを試す等すると良いかも。\n","---\ntitle: 'bashプロンプトメモ'\ndescription: 'Git のブランチをプロンプトに表示したかったので'\nslug: ps1\ndate: 2019-04-29 00:00:53\ncategory: 'Tech Blog'\ntags: [shellscript, bash]\nkeyVisual: https://i.gyazo.com/1aa55461979ca2a5e392e0bf6be39425.png\n---\n\nGit のブランチをプロンプトに表示したかったので，[プロンプトをカスタマイズして git ブランチを表示する](https://qiita.com/caad1229/items/6d71d84933c8a87af0c4)をもとに，`~/.bashrc`の`PS1`(The primary prompt string) を変更したので備忘録です\n\n<script src=\"https://gist.github.com/atsukoba/369f8afa9bde30ceafce2d4f3b087a2c.js\"></script>\n\n以上を設定し，\n\n![my prompt](https://i.gyazo.com/1aa55461979ca2a5e392e0bf6be39425.png)\n\nこうなった。\n\n---\n\n## 備忘録\n\n[2.5. Bash Prompt Escape Sequences](http://tldp.org/HOWTO/Bash-Prompt-HOWTO/bash-prompt-escape-sequences.html)によると，以下の通り。\n\n- `\\h` => ホスト名\n- `\\u` => ユーザ名\n- `\\w` => ディレクトリ（フルパス）\n- `\\W` => ディレクトリ\n- `\\t` => 時間 (24 形式)\n- `\\T` => 時間 (12 形式)\n- `\\@` => AM / PM\n- `\\d` => 日付\n- `\\D` => 日時\n- `\\#` => コマンド番号\n- `\\!` => ヒストリ番号\n- `\\n` => 改行\n\n表示としては，\n\n```bash\n[<コマンド番号>(<ヒストリ番号>)] <時間 HH:MM:SS> <user> at <directory> [<branch>]\n```\n\nとなる。\n\n`<branch>`の部分では，`parse_guit_branch`を呼んでいて，その内部がでは`git branch --no-color`の結果を`sed`で置換，エラー(`2`)を[`/dev/null`](https://ja.wikipedia.org/wiki//dev/null)へ捨てている。\n\nまた，上記記事にもあるが，色設定として以下の変数化も使える\n\n```shellscript\nlocal  BLUE=\"\\[\\e[1;34m\\]\"\nlocal  RED=\"\\[\\e[1;31m\\]\"\nlocal  GREEN=\"\\[\\e[1;32m\\]\"\nlocal  WHITE=\"\\[\\e[00m\\]\"\nlocal  GRAY=\"\\[\\e[1;37m\\]\"\n```\n\n---\n\n### ちなみに\n\n`PS2`(The secondary prompt string)も設定できるみたいだが面倒なので放置。\n","---\ntitle: 'pykakasi, mecab-python3等のメモ。'\ndescription: 'pykakasi, mecab-python3等のメモ。'\nslug: pykakasi-mecab-python\ndate: 2019-07-15 04:54:40\ncategory: 'Tech Blog'\ntags: [Python, Qiita]\nkeyVisual: https://i.gyazo.com/1b67640d65d11ebc36655c0296b6286e.png\n---\n\n## packages\n\n- requirements.txt\n\n```txt\npykakasi==1.0\nmecab-python3==0.7\npython-Levenshtein==0.12.0\n```\n\n---\n\n## pykakasi\n\n- usage\n\n`H`が hiragana, `K`が katakana, `A`が alphabet\n\n```python\nimport pykakasi.kakasi as kakasi\n\nkakasi = kakasi()\nkakasi.setMode(\"H\",\"a\") # default: Hiragana -> Roman\nkakasi.setMode(\"K\",\"a\") # default: Katakana -> Roman\nkakasi.setMode(\"J\",\"a\") # default: Japanese -> Roman\nkakasi.setMode(\"r\",\"Hepburn\") # default: use Hepburn Roman table\nkakasi.setMode(\"s\", True) # default: Separator\nkakasi.setMode(\"C\", True) # default: Capitalize\nconv = kakasi.getConverter()  # instantiate Converter\nresult = conv.do(text)  # romanize\n```\n\n---\n\n## mecab-python\n\n[MeCab](https://taku910.github.io/mecab/)の Python ラッパ。\n\n### MeCab on docker\n\n- Dockerfile\n\n```Dockerfile\nRUN apt-get update \\\n    && apt-get install -y mecab \\\n    && apt-get install -y libmecab-dev \\\n    && apt-get install -y mecab-ipadic-utf8\\\n    && apt-get install -y git\\\n    && apt-get install -y make\\\n    && apt-get install -y curl\\\n    && apt-get install -y xz-utils\\\n    && apt-get install -y file\\\n    && apt-get install -y sudo\\\n    && apt-get install -y wget\n\nRUN git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\\\n    && cd mecab-ipadic-neologd\\\n    && bin/install-mecab-ipadic-neologd -n -y\n\nRUN apt-get install -y software-properties-common vim\nRUN add-apt-repository ppa:jonathonf/python-3.6\nRUN apt-get update\n\nRUN apt-get install -y build-essential python3.6 python3.6-dev python3-pip python3.6-venv\nRUN python3.6 -m pip install pip --upgrade\nRUN pip install mecab-python3\n```\n\n- 出力のフォーマット\n\n`表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音`\n\n#### 出力結果処理\n\n```python 入力文\nimport MeCab\ntext = \"慶應義塾大学湘南藤沢キャンパス\"\nT = MeCab.Tagger(\"\")\n```\n\n- 表層系 + その他情報のタプル\n\n```python\nparsed = [[l.split('\\t')[0], tuple(l.split('\\t')[1].split(','))] for l in T.parse(text).splitlines()[:-1]]\n```\n\n```python 結果\n[['慶應義塾', ('名詞', '固有名詞', '組織', '*', '*', '*', '慶應義塾', 'ケイオウギジュク', 'ケイオーギジュク')],\n ['大学', ('名詞', '一般', '*', '*', '*', '*', '大学', 'ダイガク', 'ダイガク')],\n ['湘南', ('名詞', '固有名詞', '地域', '一般', '*', '*', '湘南', 'ショウナン', 'ショーナン')],\n ['藤沢', ('名詞', '固有名詞', '地域', '一般', '*', '*', '藤沢', 'フジサワ', 'フジサワ')],\n ['キャンパス', ('名詞', '一般', '*', '*', '*', '*', 'キャンパス', 'キャンパス', 'キャンパス')]]\n```\n\n- 全情報のタプル\n\n`re`で一気に分ける\n\n```python\nimport re\nparsed = [tuple(re.split(r\"[\\t,]\", l)) for l in T.parse(text).splitlines()[:-1]]\n```\n\n```python 結果\n[('慶應義塾', '名詞', '固有名詞', '組織', '*', '*', '*', '慶應義塾', 'ケイオウギジュク', 'ケイオーギジュク'),\n ('大学', '名詞', '一般', '*', '*', '*', '*', '大学', 'ダイガク', 'ダイガク'),\n ('湘南', '名詞', '固有名詞', '地域', '一般', '*', '*', '湘南', 'ショウナン', 'ショーナン'),\n ('藤沢', '名詞', '固有名詞', '地域', '一般', '*', '*', '藤沢', 'フジサワ', 'フジサワ'),\n ('キャンパス', '名詞', '一般', '*', '*', '*', '*', 'キャンパス', 'キャンパス', 'キャンパス')]\n```\n\n#### MeCab: 分かち書き\n\n```python\nimport MeCab\nwakati = MeCab.Tagger(\"-Owakati\")\nwakati.parse(\"慶應義塾大学湘南藤沢キャンパス\").split()\n```\n\n#### Chasen スタイル\n\n```python\nchasen = MeCab.Tagger(\"-Ochasen\")\nprint(chasen.parse(\"pythonが大好きです\"))\n```\n\n```text\npython　python　　python　名詞-固有名詞-組織\nが　　　ガ　　　　が　　　助詞-格助詞-一般\n大好き　ダイスキ　大好き　名詞-形容動詞語幹\nです　　デス　　　です　　助動詞　特殊・デス　基本形\nEOS\n```\n\n#### その他\n\n- `-Oyomi`オプションで読みの出力。ただ分かち書きがされない。\n","---\ntitle: 'ブラウザ上で録音できるツールをflask + recorder.js + p5.js on TypeScript で作る'\ndescription: 'Web Audio APIのラッパーであるrecorder.jsを用いて簡易レコーダーを作成します。ブラウザ版Processingであるp5.jsをtsで書いてUI実装します。'\nslug: ts-p5-webapp\ndate: 2019-11-4 01:27:35\ncategory: 'Tech Blog'\ntags: [p5.js, Recorder.js, Web Audio API, TypeScript]\nkeyVisual: https://i.gyazo.com/27a0651837c877d61d9c60a83bdf282a.jpg\n---\n\nWeb Audio API のラッパーである recorder.js を用いて簡易レコーダーを作成します。ブラウザ版 Processing である p5.js を ts で書いて UI 実装します。\n\n> _フィールドレコーディング：スタジオ外での自然音や環境音の録音_\n> 自然音や環境音を手軽に集めたい，そしてそれを PC へ送りリアルタイムに処理したい，といったニッチな要望に応えるものを作った感じです\n\n## 完成イメージ\n\nイメージといってもスクリーンショットなんでこういう感じで動きます。\n\n![animation screenshot](https://i.gyazo.com/6825cb4c65c8d1c4e7f8f7a3a6a357d4.gif)\n\n(たぶん)ササっと環境構築して動かせるので興味ある方は是非。\n\n## recorderjs でフロント側で音声録音する\n\nGitHub: <https://github.com/mattdiamond/Recorderjs>\n\nまずデモはこちら[Simple Recorder.js demo](https://addpipe.com/simple-recorderjs-demo/)\n\nWeb Audio API のラッパーみたいな感じでしょうか。AudioNode のインスタンスを渡せば簡単に録音スタート・ストップ・保存ができる，という優れモノ。\n\n以下のような感じで録音開始の関数定義ができるので，任意のイベントで呼べば良い。\n\n```typescript\nlet recorder: Recorder\n\nconst startUserMedia = (stream: MediaStream) => {\n  audio_context = new AudioContext()\n  let input: AudioNode = audio_context.createMediaStreamSource(stream)\n  recorder = new Recorder(input)\n}\n\nconst startRecording = () => {\n  recorder && recorder.record()\n}\n```\n\nで，この`Recorder.exportWAV()`メソッド一発で wav の Blob オブジェクトが手に入るので，ソレを ajax で POST してあげれば良い。\n\n```typescript\nrecorder &&\n  recorder.exportWAV((blob: Blob) => {\n    let url = URL.createObjectURL(blob)\n    let fd = new FormData()\n    fd.append('data', blob)\n    $.ajax({\n      type: 'POST',\n      url: '/',\n      data: fd,\n    }).done((data) => {\n      recorder.clear()\n    })\n  })\n```\n\n## flask で POST された wav ファイルを保存する\n\nflask 側ではこんな感じに書けば良い。\n\n```python\nfrom flask import Flask, jsonify, request\n\n\n@app.route('/', methods=['POST'])\ndef uploaded_wav():\n    fname = \"sounds/\" + datetime.now().strftime('%m%d%H%M%S') + \".wav\"\n    with open(f\"{fname}\", \"wb\") as f:\n        f.write(request.files['data'].read())\n    print(f\"posted sound file: {fname}\")\n    return jsonify({\"data\": fname})\n```\n\nこれで`sounds/`直下に`1104235900.wav`みたいなファイルがどんどん溜まっていく。\n\n## 保存されたファイルのパスを osc で送る\n\n個人的にこのアプリケーションをパフォーマンスで使用したいので，サウンドファイルが保存されたタイミングで osc にメッセージを飛ばしてみる。コレで例えばサーバとなっているローカルの PC で Max/MSP や Max for Live を用いたリアルタイムでのサウンドファイル読み込みがラクになる（と信じている）\n\n`pythonosc`というパッケージを用いる。(`pip install python-osc`で入る)\n\npython-osc PyPI: <https://pypi.org/project/python-osc/>\n\n```python\nfrom pythonosc import dispatcher, osc_message_builder, osc_server, udp_client\n\n\naddress = \"127.0.0.1\"\nport = 5050\nclient = udp_client.UDPClient(address, port)\n\n\ndef send_osc(msg):\n    msg_obj = osc_message_builder.OscMessageBuilder(address=address)\n    msg_obj.add_arg(msg)\n    client.send(msg_obj.build())\n```\n\nこれで良い。あとは上述の`uploaded_wav()`内で`send_osc(fname)`してあげれば，ファイルパスがメッセージとして届く。Max なら`[udpreceive 5050]`しておけば open&sfplay~して再生できる。\n\n## p5.js\n\np5js.org: <https://p5js.org/>\n\nDOM がいじれる Processing という感じで，Canvas 要素に描画するので CSS で複雑なアニメーションを描いているとかしなくても，canvas が動くブラウザなら良いしこっちのがラクかもしれないです。また，Web Editor(<https://editor.p5js.org/>) というものがあり，環境構築ナシで挙動が試せるので非常にとっかりやすいと思います。\n\nTypeScript を導入するなら，まず以下のリポジトリを使うべきです（めっちゃラクだった）\n\n<blockquote class=\"embedly-card\"><h4><a href=\"https://github.com/Gaweph/p5-typescript-starter\">Gaweph/p5-typescript-starter</a></h4><p>Base starter project using p5js and typescript: Contribute to Gaweph/p5-typescript-starter development by creating an account on GitHub.</p></blockquote>\n\nかつ，以下のエントリを参考にしました\n\n- [TypeScript+webpack で Processing(p5.js)の環境を構築する - Qiita](https://qiita.com/uchiko/items/744d7559d37973a959ea)\n- [CreativeCoding 用に P5.js が TypeScript で書ける環境をつくった。 - Qiita](https://qiita.com/y___k/items/429e7095ef638a515b07)\n\nあとは，ササっと書いていくだけです。例として UI の録音ボタンの部分のクラスをおいておきます…\n\n```typescript\nclass Button {\n  private w: number\n  private h: number\n  private centerX: number\n  private centerY: number\n  private radius: number\n  private isRecording: boolean\n  private rectCircleRatio: number\n  private progress: number // 0 ~ 300 value (about 5s)\n\n  constructor(w: number, h: number, size: number) {\n    this.w = w\n    this.h = h\n    this.centerX = w / 2\n    this.centerY = h / 2\n    this.radius = size\n    this.isRecording = false\n    this.rectCircleRatio = size / 2\n    this.progress = 0\n  }\n\n  isTouched(x: number, y: number) {\n    if ((x - this.centerX) ** 2 + (y - this.centerY) ** 2 < this.radius ** 2) {\n      return true\n    }\n    return false\n  }\n\n  switchRecording() {\n    this.isRecording = !this.isRecording\n    console.log(`switched to recording: ${this.isRecording}`)\n    if (this.isRecording) {\n      startRecording()\n    } else {\n      this.progress = 0\n      stopRecording()\n    }\n  }\n\n  draw() {\n    if (this.progress == 300) {\n      this.progress = 0\n      this.switchRecording()\n    }\n    if (this.isRecording) {\n      if (this.rectCircleRatio > 5) {\n        clear()\n        this.rectCircleRatio -= 5\n      }\n      this.progress++\n    } else {\n      if (this.rectCircleRatio <= this.radius / 2) {\n        clear()\n        this.rectCircleRatio += 5\n      }\n    }\n    drawCircleUI((this.progress * 2 * PI) / 300)\n    noStroke()\n    fill(mainColor)\n    rect(\n      this.centerX - this.radius / 2,\n      this.centerY - this.radius / 2,\n      this.radius,\n      this.radius,\n      this.rectCircleRatio,\n    )\n    // text\n    fill(white)\n    textAlign(CENTER, CENTER)\n    textSize(16)\n    if (this.isRecording) {\n      text('STOP', this.centerX, this.centerY)\n    } else {\n      text('REC', this.centerX, this.centerY)\n    }\n  }\n}\n```\n\n## リポジトリ\n\n<blockquote class=\"embedly-card\" data-card-controls=\"0\"><h4><a href=\"https://github.com/atsukoba/AudioSampleRecorder\">atsukoba/AudioSampleRecorder</a></h4><p>Audio recording on the Web using Web Audio API for remote real-time environmental sound collection. python3 and packages listed in requirements.txt nodejs and packages listed in package.json tmux ngrok recorder-js git clone -r https://github.com/atsukoba/AudioSampleRecorder.git npm install sh ngrok-install.sh then put your ngrok auth-token if using pip and HomeBrew, run this prepared script.</p></blockquote>\n<script async src=\"//cdn.embedly.com/widgets/platform.js\" charset=\"UTF-8\"></script>\n\n実際に活用できるので気が向いたらどうぞ。[osc-webapp]()と同じく，ngrok で https トンネルほって公開してます。(https じゃないと Web Audio API が使えない)\n","---\ntitle: 'WordPressのTips（ログイン画面変更・最適化などのメモ）'\ndescription: 'ログイン画面変更，SEO，画像最適化と高速化などでとりあえずやること'\nslug: wordpress-tips\ndate: 2019-8-04 00:26:20\ncategory: 'Tech Blog'\ntags: [WordPress]\nkeyVisual: https://i.gyazo.com/abc431092a83b10dfaffb2f883590bef.png\n---\n\nログイン画面変更，SEO，画像最適化と高速化などでとりあえずやることを書いておく\n\n## ログイン画面変更\n\n```php\n// add to functions.php\nfunction custom_login()\n{\n  $style = '\n  <style>\n    .login > #login > h1 > a {\n      background-image: url(PATH/TO/IMAGE.png);\n      background-size: 100%;\n      width: 100%;\n      height: 100px;\n    }\n    .login > #login > h1::after {\n      content: \"LOGIN DESCRIPTION\";\n      font-size: 1rem;\n    }\n  </style>\n  ';\n  echo $style;\n}\nadd_action('login_enqueue_scripts', 'custom_login');\n```\n\n## SEO\n\n`All in One SEO Pack`で meta タグ挿入，<https://search.google.com/search-console>での検索最適化\n\n- sitemap の登録と ping\n\n`XML Sitemap Generator for WordPress`を用いるが，この際に「html 形式のサイトマップを含める」のチェックを外すこと（search console 上で xml 外の形式だとエラーが出るため）\n\n## サイト高速化\n\n<https://developers.google.com/speed/pagespeed/insights> でのスピード診断を行い，SP/PC 両方での速度の向上を図る。 WordPress では高速化のためのプラグインが複数あるため，そのうち今回導入した事例を示す\n\n### imagify\n\npagespeed insights の診断結果で「次世代フォーマットでの画像の配信」が上位に来る時，サイト内で読み込んでいる画像のサイズが大きさを減らせば大幅な改善が期待できる。imagify というプラグインでは，メディアライブラリ上の画像ファイルを（サムネイル等の別サイズも含め）自動的に縮小してくれるため，非常に有効。\n\n画質が気にならない場合はすべて Ultra モードで縮小化しても構わない。背景画像等はそもそもの解像度も小さくする（resize）する\n\n### Asset CleanUp\n\npagespeed insights の診断結果で「レンダリングを妨げるリソースの除外」が上位に来る場合は外部ファイルの読み込み時間を減らすことで大幅な改善が期待できる。`Asset CleanUp`を用いて以下の処置をし，速度が改善したので示す。\n\n- css の縮小化と結合\n- 使っていない css の除去\n- Google Fonts の CDN リクエストが複数存在する際の一本化&async にする\n\n(ただしフロント開発の際は Sass の map とかも無効化され可読性が著しく下がるので開発終了後に行う)\n"],"title":"Nextjs Blog Site","description":"A Simple Markdown Blog build with Nextjs."},"__N_SSG":true}