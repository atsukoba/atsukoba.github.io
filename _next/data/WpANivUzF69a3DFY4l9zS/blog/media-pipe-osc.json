{"pageProps":{"content":"\nMediapipe: <https://github.com/google/mediapipe>\n\nMediaPipe Hand: <https://google.github.io/mediapipe/solutions/hands>\n\n---\n\nMediaPipeはBazarevskyらがCVPR2019で発表したオープンソースの機械学習用フレームワークで，そこで用いられている手認識機能は，single-shot手のひら認識アルゴリズムとlandmark認識モデルが組み合わされたものです（[Google AI Blog: On-Device, Real-Time Hand Tracking with MediaPipe](https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html)）\n\n[![Image from Gyazo](https://i.gyazo.com/bb83a5b6ca5f6ca7865836d9f1e9e3d7.jpg)](https://gyazo.com/bb83a5b6ca5f6ca7865836d9f1e9e3d7)\n\n手の形状は以下の各ランドマークの座標として取得できます．毎フレーム推論が走り，cv座標での値が取得できます．\n\n[![Image from Gyazo](https://i.gyazo.com/d4a41006b4110401a6fb593da4c4d544.png)](https://gyazo.com/d4a41006b4110401a6fb593da4c4d544)\n\n返ってくる`landmark`のオブジェクトは以下のようにして座標の`float`が取り出せます\n\n```python\nfor hand_idx, landmarks in enumerate(multi_hand_landmarks):\n    for point_idx, points in enumerate(landmarks.landmark):\n        print(f\"Hand: {hand_idx}, {HAND_LANDMARK_NAMES[point_idx]},\"\n                      + f\"x:{points.x} y:{points.y} z:{points.z}\")\n```\n\nこの時の`HAND_LANDMARK_NAMES`は，以下のような順番になっています．\n\n```python\nHAND_LANDMARK_NAMES = [\n    \"wrist\",\n    \"thumb_1\",\n    \"thumb_2\",\n    \"thumb_3\",\n    \"thumb_4\",\n    \"index_1\",\n    \"index_2\",\n    \"index_3\",\n    \"index_4\",\n    \"middle_1\",\n    \"middle_2\",\n    \"middle_3\",\n    \"middle_4\",\n    \"ring_1\",\n    \"ring_2\",\n    \"ring_3\",\n    \"ring_4\",\n    \"pinky_1\",\n    \"pinky_2\",\n    \"pinky_3\",\n    \"pinky_4\"\n]\n```\n\n今回はこのMediaPipeによるリアルタイム手認識を用いて何かしらのインタラクティブ作品やWekinator等を用いるジェスチャ認識などのためのOpen Sound Controlでのデータ送信をプロトタイプします．\n\n## スクリプト\n\n```python\n# Atsuya Kobayashi 2020-12-22\n# Reference: https://google.github.io/mediapipe/solutions/hands\n# LICENCE: MIT\n\nfrom itertools import chain\n\nimport mediapipe as mp\nfrom cv2 import cv2\nfrom pythonosc import udp_client\n\nIP = \"127.0.0.1\"\nPORT = 7474\nVIDEO_DEVICE_ID = 0\nRELATIVE_AXIS_MODE = True\n\nHAND_LANDMARK_NAMES = [\n    \"wrist\",\n    \"thumb_1\",\n    \"thumb_2\",\n    \"thumb_3\",\n    \"thumb_4\",\n    \"index_1\",\n    \"index_2\",\n    \"index_3\",\n    \"index_4\",\n    \"middle_1\",\n    \"middle_2\",\n    \"middle_3\",\n    \"middle_4\",\n    \"ring_1\",\n    \"ring_2\",\n    \"ring_3\",\n    \"ring_4\",\n    \"pinky_1\",\n    \"pinky_2\",\n    \"pinky_3\",\n    \"pinky_4\"\n]\n\n\ndef extract_detected_hands_points(multi_hand_landmarks,\n                                  send_osc_client=None):\n\n    if multi_hand_landmarks is not None:\n        for hand_idx, landmarks in enumerate(multi_hand_landmarks):\n            for point_idx, points in enumerate(landmarks.landmark):\n\n                # if you want to check data on console\n                print(f\"Hand: {hand_idx}, {HAND_LANDMARK_NAMES[point_idx]},\"\n                      + f\"x:{points.x} y:{points.y} z:{points.z}\")\n                \"\"\"\n                if you want to send data to addresses correspoding\n                to landmarks names on detected hands, use berow\n                \"\"\"\n                # if send_osc_client is not None:\n                #     send_osc_client.send_message(f\"/{HAND_LANDMARK_NAMES[point_idx]}\",\n                #                                  [points.x, points.y])\n\n            \"\"\"if you want to send data to single input address, use berow\"\"\"\n            if send_osc_client is not None:\n                send_osc_client.send_message(\n                    f\"/YOUR_OSC_ADDRESS\",\n                    list(chain.from_iterable([[p.x, p.y] for p in landmarks.landmark])))\n\n\nif __name__ == \"__main__\":\n\n    mp_drawing = mp.solutions.drawing_utils\n    mp_hands = mp.solutions.hands\n\n    hands = mp_hands.Hands(\n        min_detection_confidence=0.5, min_tracking_confidence=0.5)\n\n    cap = cv2.VideoCapture(VIDEO_DEVICE_ID)\n\n    osc_client = udp_client.SimpleUDPClient(IP, PORT)\n\n    while cap.isOpened():\n        success, image = cap.read()\n        if not success:\n            print(\"Ignoring empty camera frame.\")\n            # If loading a video, use 'break' instead of 'continue'.\n            continue\n\n        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n        image.flags.writeable = False\n        results = hands.process(image)\n        extract_detected_hands_points(results.multi_hand_landmarks,\n                                      send_osc_client=osc_client)\n        image.flags.writeable = True\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                mp_drawing.draw_landmarks(\n                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n        cv2.imshow('Detected Hands', image)\n\n        if cv2.waitKey(5) & 0xFF == 27:\n            break\n\n    hands.close()\n    cap.release()\n```\n","data":{"title":"MediaPipeによるお手軽手認識とリアルタイムおもちゃのためのOSC連携","description":"","slug":"media-pipe-osc","date":"2020-12-22T00:26:20.000Z","category":"Tech Blog","tags":["MediaPipe","Open Sound Control"],"keyVisual":"https://i.gyazo.com/229d4f1e990ac46f3d4e4b5dfd9806c3.jpg"}},"__N_SSG":true}