{"pageProps":{"content":"\nThis work was exhibited on ‘MUSES EX MACHINA’ by TOKUI Nao Computational Creativity Lab, Keio University, December 9, 2022 -- January 15, 2023, at NTT Intercommunication Center [ICC], Tokyo.\n\n<https://www.ntticc.or.jp/en/exhibitions/2022/tokui-nao-computational-creativity-lab-keio-university/>\n\n---\n\nIn this work, my role was mainly management of the engineering team and development of the system that controls several deep learning models.\n\n[![Image from Gyazo](https://i.gyazo.com/e8f6f32bd9e1e5606d615e68626c4b1a.jpg)](https://gyazo.com/e8f6f32bd9e1e5606d615e68626c4b1a)\n\n## Concept\n\nIn this work, two AI agents repeat both the generation of pictures and the utterance of interpretations alternately. One AI converts the descriptions of the pictures it generates into sentences and speaks them out loud, while the other AI listens, generates the next picture based on it, and speaks in the same way. The newly generated pictures are interpreted and spoken, resulting in a creative Echo.\n\n[![Image from Gyazo](https://i.gyazo.com/91dfa3f470b4f6866f62ab7598baf028.jpg)](https://gyazo.com/91dfa3f470b4f6866f62ab7598baf028)\n\nCurrent image-generating AIs have learned from the pictures and aesthetics that humans have created. The AI-generated pictures are reverberations, or Echoes, of human creativity in the learning data. The generated pictures are eventually disseminated on the Web and used by AI as training data. At this point, although the generated images may seem novel, they can actually be seen as stuck within the Echo that has existed up to that point.\nThis \"inside Echo\" can also be said of us humans. The everyday productions are the result of past creations, just like the Echoes above. With this chain of Echoes, people live in the past and now Echo into the next age.\nHowever, doesn't the next age, or the future, seem to be different from the past age, \"inside Echo\"? Because now there are other things that emit Echoes besides us humans.\nThe AIs here are not only listening to each other's speech, but also to external noises, such as human voices and environmental sounds. It is not clear whether these AIs hate it or admire it, but what is certain is that we can influence each other. And beyond that, a different Echo could resonate.\n\nWhat will be created beyond the point where \"all\" our Echoes resonate with each other?\n\n---\n\nSupervisor: TOKUI Nao  \nTechnical Director: KOBAYASHI Atsuya  \nConcept Director: KOBAYASHI Yuga  \nOriginal Concept: Ryo SIMON  \nLighting: OKAZAKI Keisuke, TAKAISHI Keito, SHIBUYA Kazufumi  \nMachine Learning: ISHII Asuka, SAWA Shoma  \nSound: Ryo SIMON, TAKANASHI Dai, OBARA Kai  \nVisual: TAKAISHI Keito, SHIBUYA Kazufumi, ISHII Asuka, MATSUOKA Yuma  \nConcept: HANDA Sogen, NOBUSUE Ryuku, OKAZAKI Keisuke, INOUE Takumi  \nSupport: NARUSE Santa, KIEU Quoc Thai, SASAKI Yuria  \n","data":{"description":"","title":"Muses Ex Echoes","slug":"muses-ex-echoes","date":"2022-12-15T12:00:00.000Z","category":"Installation","tags":["AI","Audio Visual","Geneative Models"],"keyVisual":"/images/works/muses-ex-echoes.jpg"}},"__N_SSG":true}