{"pageProps":{"data":["---\ndescription: \"\"\ntitle: Audio recording on the Web using Web Audio API\nslug: audio-sample-recorder\ndate: 2020-4-6 00:00:16\ncategory: Work\ntags: [Web, Audio, Tool]\nkeyVisual: /images/works/audio-sample-recorder.png\n---\n","---\ndescription: \"'Drive and listen' is an experimental work aiming to interpreting cities through sound by trafic sonification.\"\ntitle: \"Drive-and-Listen\"\nslug: drive-and-listen\ndate: 2020-7-15 23:20:16\ncategory: Prototype\ntags: [Sonification]\nkeyVisual: https://i.gyazo.com/3b325bfa825898b7d7d31b95c97c52cf.jpg\n---\n\n<div class=\"iframe-video-wrapper\">\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BlPp1gVSaEQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nWe developed an inference environment for object recognition models using YOLO (v2, v3) (Darknet) and unique OSC sequencer for playing in CPU environment. Then sort (non-maximum suppresion) each unique object recognized to convert into sounds. The player is implemented with MIDI connection between Max and Ableton Live.\n\n[![Image from Gyazo](https://i.gyazo.com/7231b69bed264b915924c4377bc2e2aa.png)](https://gyazo.com/7231b69bed264b915924c4377bc2e2aa)\n\nAtsuya Kobayashi  \nTakumi Inoue  \nKeisuke Okazaki  \nYoriaki Hirota\n","---\ndescription: \"\"\ntitle: \"Echoes for unknown egos ― manifestations of sound\"\nslug: echoes-for-unknown-egos\ndate: 2022-6-4 23:20:16\ncategory: Performance\ntags: [Music, AI, Works, Performance]\nkeyVisual: /images/works/echoesforunknownegos.jpg\n---\n\nThis is an improvisation percussion performance designed as a joint project of Japanese Drummer/Percussionist  [Shun Ishiwaka](https://www.shun-ishiwaka.com/) and [YCAM (Yamaguchi Center for Art and Media) InterLab](https://special.ycam.jp/interlab/en.html). I was in charge of the development of several software systems used in the performance. We have taken several approaches to extend the musical creativity in improvisation by creating \"alter egos\" of Ishiwaka. My work in this project was developing some Agents that respond to Ishiwaka's play in real time, and \"Meta-Agents\" that observe the musical state of Ishiwaka's play and orchestrate other Agents based on the situation.\n\n## Melody Agent\n\n*Melody* agent plays generated melody in real-time along with Ishiwaka's drums play.\n\n[![Image from Gyazo](https://i.gyazo.com/6acbb5bf0b6b2f5b16a4a4c2ffca7388.jpg)](https://gyazo.com/6acbb5bf0b6b2f5b16a4a4c2ffca7388)\n\n## Sampler Agent\n\n**Sampler** agent plays recorded samples of Ishiwaka's past Piano play along with drums play.\n\n[![Image from Gyazo](https://i.gyazo.com/4c059f470d1506f1144ec5e0463c8e8b.gif)](https://gyazo.com/4c059f470d1506f1144ec5e0463c8e8b)\n\n## Meta Agent\n\n**Meta-Agent** is an agent that controls other agents play style and designs combination of agents.\n\n[![Image from Gyazo](https://i.gyazo.com/6e06641f939c6d6215cb96ae0938ca38.gif)](https://gyazo.com/6e06641f939c6d6215cb96ae0938ca38)\n\n## Teaser\n\n<div class=\"iframe-video-wrapper\">\n    <iframe class=\"iframe-video\" title=\"vimeo-player\" src=\"https://player.vimeo.com/video/701264303?h=dd404cde3c\" \n    frameborder=\"0\" allowfullscreen></iframe>\n</div>\n\n## Echoes for unknown egos―manifestations of sound (Day1 / Excerpt)\n\n<div class=\"iframe-video-wrapper\">\n    <iframe src=\"https://player.vimeo.com/video/741153714?h=7700f875f6\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Echoes for unknown egos―manifestations of sound (Day2 / Excerpt)\n\n<div class=\"iframe-video-wrapper\">\n    <iframe src=\"https://player.vimeo.com/video/741154438?h=da4028b79f\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Links\n\n[Echoes for unknown egos―発現しあう響きたち](https://www.ycam.jp/events/2022/echoes-for-unknown-egos/)  \n[Echoes for unknown egos ― manifestations of sound (en)](https://www.ycam.jp/en/events/2022/echoes-for-unknown-egos/)","---\ntitle: 'ExSampling: a system for the real-time ensemble performance of field-recorded environmental sounds'\ndescription: 'We propose ExSampling: an integrated system of recording application and Deep Learning environment for a real-time music performance of environmental sounds sampled by field recording. Automated sound mapping to Ableton Live tracks by Deep Learning enables field recording to be applied to real-time performance, and create interactions among sound recorders, composers and performers.'\nslug: exsampling\ndate: 2020-5-13 23:20:16\ncategory: Research\ntags: [NIME, Music Tech]\nkeyVisual: /images/works/exsampling.png\n---\n\nWe propose ExSampling: an integrated system of recording application and Deep Learning environment for a real-time music performance of environmental sounds sampled by field recording. Automated sound mapping to Ableton Live tracks by Deep Learning enables field recording to be applied to real-time performance, and create interactions among sound recorders, composers and performers.\n\nPaper: <https://arxiv.org/abs/2006.09645>\n\nOur lab: <https://cclab.sfc.keio.ac.jp/2020/06/16/exsampling/>\n\n---\n## System Overview\n\n[![Image from Gyazo](https://i.gyazo.com/2fd312a37e3f52afcbcedada0074bd0b.png)](https://gyazo.com/2fd312a37e3f52afcbcedada0074bd0b)\n\n[![Image from Gyazo](https://i.gyazo.com/f0207f9163e18db20be2aca9d9fd34ae.png)](https://gyazo.com/f0207f9163e18db20be2aca9d9fd34ae)\n\n## Classification Architecture\n\n[![Image from Gyazo](https://i.gyazo.com/5cd02c02588b182bc60b06d718b480ba.png)](https://gyazo.com/5cd02c02588b182bc60b06d718b480ba)\n\n## MIDI Track Mapping\n\n[![Image from Gyazo](https://i.gyazo.com/9cb0e946be5ab235547a0b3e03b9f174.png)](https://gyazo.com/9cb0e946be5ab235547a0b3e03b9f174)\n\n## Demo videos\n\n<div class=\"iframe-video-wrapper\">\n    <iframe title=\"vimeo-player\" src=\"https://player.vimeo.com/video/429487962\" width=\"640\" height=\"400\" frameborder=\"0\" allowfullscreen></iframe>\n</div>\n\n<div class=\"iframe-video-wrapper\">\n    <iframe title=\"vimeo-player\" src=\"https://player.vimeo.com/video/429487994\" width=\"640\" height=\"360\" frameborder=\"0\" allowfullscreen></iframe>\n</div>\n\nBibTex\n\n```bibtex\n@inproceedings{kobayashi_NIME20_58,\n  author = {Kobayashi, Atsuya and Anzai, Reo and Tokui, Nao},\n  title = {ExSampling: a system for the real-time ensemble performance of field-recorded environmental sounds},\n  pages = {305--308},\n  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},\n  editor = {Michon, Romain and Schroeder, Franziska},\n  year = {2020},\n  month = jul,\n  publisher = {Birmingham City University},\n  address = {Birmingham, UK},\n  issn = {2220-4806},\n  doi = {10.5281/zenodo.4813371},\n  url = {https://www.nime.org/proceedings/2020/nime2020_paper58.pdf}\n}\n```\n","---\ntitle: \"Applying wah effect based on face tilt with 'faceWah'\"\ndescription: \"\"\nslug: faceWah\ndate: 2020-6-20 23:20:16\ncategory: Research\ntags: [Music, HCI]\nkeyVisual: /images/works/facewah.png\n---\n\n<https://github.com/atsukoba/faceWah>\n","---\ndescription: \"Feature-Dive is a web-based user interface for searching songs by exploring feature in 3D space.\"\ntitle: \"Feature-Dive\"\nslug: feature-dive\ndate: 2022-8-27 12:00:00\ncategory: Interaction Design\ntags: [Music, Visualization, Interface]\nkeyVisual: /images/works/feature-dive.jpg\n---\n\nThis is an application of searching songs with voth audio and symbolic features extracted from song. Using manifold dimensionality reduction techniques, I designed interactive experience of exploring the space of multiple features.\n\n[![Image from Gyazo](https://i.gyazo.com/7f38a90fda214f97efedeb52202429a0.gif)](https://gyazo.com/7f38a90fda214f97efedeb52202429a0)\n\n[![Image from Gyazo](https://i.gyazo.com/fe9d7c4308fe438b29c855501bc645cb.gif)](https://gyazo.com/fe9d7c4308fe438b29c855501bc645cb)\n\n[![Image from Gyazo](https://i.gyazo.com/dea8e83ac50b7e6febdff30f5fe06df0.gif)](https://gyazo.com/dea8e83ac50b7e6febdff30f5fe06df0)\n\n\n","---\ndescription: \"Improvise+=Chain [2022] is an audio-visual installation work of listening to AI's improvisation ensemble.\"\ntitle: \"Improvise+=Chain\"\nslug: improvise-chain\ndate: 2022-11-15 12:00:00\ncategory: Installation\ntags: [Music, AI, Works, Performance]\nkeyVisual: /images/works/improvise-chain.jpg\n---\n\n<div class=\"iframe-video-wrapper\">\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/wiFhfswgsMU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n</div>\n\nThis work was exhibited on ‘MUSES EX MACHINA’ by TOKUI Nao Computational Creativity Lab, Keio University, November 14, 2022 -- January 15, 2023, at NTT Intercommunication Center [ICC], Tokyo.\n\n<https://www.ntticc.or.jp/en/exhibitions/2022/tokui-nao-computational-creativity-lab-keio-university/>\n\n## Concept of this Work\n\nThis work is an audiovisual installation artwork of autonomous musical performance using artificial intelligence technology, designed to provide the audience with an experience exploring the differences between human and AI-based virtual musicians.\n\n[![Image from Gyazo](https://i.gyazo.com/c20911da645271b5352a78bce9a21d67.jpg)](https://gyazo.com/c20911da645271b5352a78bce9a21d67)\n\nUsing a transformer decoder, we developed a four-track (melody, bass, chords and accompaniment, and drums) symbolic music generation model, which generates each track in real-time to create an endless chain of phrases.\n\n[![Image from Gyazo](https://i.gyazo.com/0a948b366ce79de7db8d19eb2a68b29d.png)](https://gyazo.com/0a948b366ce79de7db8d19eb2a68b29d)\n\n3D visuals and LED lights represent the Attention information calculated within the model. This work aims to highlight the differences for viewers to consider between humans and artificial intelligence in music jams by visualizing the only information virtual musicians can communicate with while humans interact in various modals during the performance.\n\n[![Image from Gyazo](https://i.gyazo.com/ec20beb9c1bee4a9040eff3917421949.png)](https://gyazo.com/ec20beb9c1bee4a9040eff3917421949)\n\nWe developed user interface for installers using Max/MSP patcher that can be used as a monitor of value for LED control and attentions.\n\n---\n\nResearch & Development: Atsuya Kobayashi  \nConcept Design: Atsuya Kobayashi  \nVisualization : Ryo Simon  \nFilming : Asuka Ishii, Kazufumi Shibuya\n","---\ndescription: \"Linguitone is a web-based system to assist in making electric guitar sounds using natural language expressions techniques.\"\ntitle: \"Linguitone\"\nslug: linguitone\ndate: 2021-3-16 23:20:16\ncategory: Research\ntags: [Music, HCI]\nkeyVisual: /images/works/linguitone.jpg\n---\n\n<div class=\"iframe-video-wrapper\">\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rVBiaRbN19U\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nI developed Web-based user intarface for designing the experience of \"controling guitar effects with text inputs\", and trained machine learning model to estimate the effect parameters from text embedding vectors using Japanese FastText / Doc2Vec and simple feed forward MLP.\n\n[![Image from Gyazo](https://i.gyazo.com/739e3b632a84f9a4f29cc0f23f1861d0.jpg)](https://gyazo.com/739e3b632a84f9a4f29cc0f23f1861d0)\n\nAudio effects are selected from Ableton Live effects controlled in real-time with text input from the application. Then the user can control smoothly while playing the guitar.\n\n[![Image from Gyazo](https://i.gyazo.com/1b666c3c0ebce68b35857187303635b6.jpg)](https://gyazo.com/1b666c3c0ebce68b35857187303635b6)\n\n## Paper\n\n[Linguitone: 音色の言語表現を用いたエレクトリックギターの音作り支援システム](https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=210226&item_no=1&page_id=13&block_id=8)\n","---\ndescription: \"A project at computational creativity lab summer camp hackathon 2019\"\ntitle: \"MemoryBody: Wearing Digital Memories\"\nslug: memorybody\ndate: 2019-8-6 23:20:16\ncategory: Work\ntags: [Interactive Art, PoseNet]\nkeyVisual: /images/works/memorybody.png\n---\n\n## Concept\n\nAs a memory, data of the past posts on SNS binds the person's own identity, behavior and the next post. We bring out the situation by make a user wear its past SNS posts data.\n\n[![Image from Gyazo](https://i.gyazo.com/4cf9e2c300dac5c030a3de74e7604bd0.png)](https://gyazo.com/4cf9e2c300dac5c030a3de74e7604bd0)\n\n[![Image from Gyazo](https://i.gyazo.com/b518fcede0d5c520f4d00b121c792213.jpg)](https://gyazo.com/b518fcede0d5c520f4d00b121c792213)\n\nSource code is available on [Google Colab](https://colab.research.google.com/drive/1qPx3D2dXl_cpTk7Zr5AxyZkcMbeJxFEw)\n","---\ndescription: \"MR4MR: Mixed Reality for Melody Reincarnation\"\ntitle: \"MR4MR: Mixed Reality for Melody Reincarnation\"\nslug: mr4mr\ndate: 2022-09-15 12:00:00\ncategory: \"Interaction Design\"\ntags: [Music, AI, XR, Mixed Reality]\nkeyVisual: /images/works/mr4mr.jpg\n---\n\n<div class=\"iframe-video-wrapper\">\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xPMbqHUlyyU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n</div>\n\nThere is a long history of an effort made to explore musical elements with the entities and spaces around us, such as musique concrète and ambient music. In the context of computer music and digital art, interactive experiences that concentrate on the surrounding objects and physical spaces have also been designed. In recent years, with the development and popularization of devices, an increasing number of works have been designed in Extended Reality to create such musical experiences. In this paper, we describe MR4MR, a sound installation work that allows users to experience melodies produced from interactions with their surrounding space in the context of Mixed Reality (MR). Using HoloLens, an MR head-mounted display, users can bump virtual objects that emit sound against real objects in their surroundings. Then, by continuously creating a melody following the sound made by the object and re-generating randomly and gradually changing melody using music generation machine learning models, users can feel their ambient melody \"reincarnating\".\n\nThis work has exhibited in NTT Intercommunication Center\n\n<https://www.ntticc.or.jp/ja/archive/works/mr4mr-mixed-reality-for-melody-reincarnation/>\n\n\n[![Image from Gyazo](https://i.gyazo.com/7866eb5bfa812c8c486078d8a7cba279.gif)](https://gyazo.com/7866eb5bfa812c8c486078d8a7cba279)\n\n## Paper\n\n[[2209.07023] MR4MR: Mixed Reality for Melody Reincarnation](https://arxiv.org/abs/2209.07023)\n\nPlease cite as\n\n_Kobayashi, Atsuya, Ishino, Ryogo, Nobusue, Ryuku, Inoue, Takumi, Okazaki, Keisuke, Sawa, Shoma, & Tokui, Nao. (2022, September 17). MR4MR: Mixed Reality for Melody Reincarnation. Proceedings of the 3rd Conference on AI Music Creativity. The 3rd Conference on AI Music Creativity (AIMC 2022). https://doi.org/10.5281/zenodo.7088357_\n\n### Bibtex\n\n```bibtex\n@inproceedings{kobayashi_mr4mr2022,\n  title        = {MR4MR: Mixed Reality for Melody Reincarnation},\n  author       = {\n    Kobayashi, Atsuya and Ishino, Ryogo and Nobusue, Ryuku and Inoue, Takumi\n    and Okazaki, Keisuke and Sawa, Shoma and Tokui, Nao\n  },\n  year         = 2022,\n  month        = {Sep},\n  booktitle    = {Proceedings of the 3rd Conference on AI Music Creativity},\n  publisher    = {AIMC},\n  doi          = {10.5281/zenodo.7088357},\n  abstractnote = {\n    <p>There is a long history of an effort made to explore musical elements\n    with the entities and spaces around us, such as musique concr&egrave;te and\n    ambient music. In the context of computer music and digital art,\n    interactive experiences that concentrate on the surrounding objects and\n    physical spaces have also been designed. In recent years, with the\n    development and popularization of devices, an increasing number of works\n    have been designed in Extended Reality to create such musical experiences.\n    In this paper, we describe MR4MR, a sound installation work that allows\n    users to experience melodies produced from interactions with their\n    surrounding space in the context of Mixed Reality (MR). Using HoloLens, an\n    MR head-mounted display, users can bump virtual objects that emit sound\n    against real objects in their surroundings. Then, by continuously creating\n    a melody following the sound made by the object and re-generating randomly\n    and gradually changing melody using music generation machine learning\n    models, users can feel their ambient melody &quot;reincarnating&quot;.</p>\n  }\n}\n\n```\n","---\ndescription: \"\"\ntitle: \"Muses Ex Echoes\"\nslug: muses-ex-echoes\ndate: 2022-12-15 12:00:00\ncategory: Installation\ntags: [AI, Audio Visual, Geneative Models]\nkeyVisual: /images/works/muses-ex-echoes.jpg\n---\n\nThis work was exhibited on ‘MUSES EX MACHINA’ by TOKUI Nao Computational Creativity Lab, Keio University, December 9, 2022 -- January 15, 2023, at NTT Intercommunication Center [ICC], Tokyo.\n\n<https://www.ntticc.or.jp/en/exhibitions/2022/tokui-nao-computational-creativity-lab-keio-university/>\n\n---\n\nIn this work, my role was mainly management of the engineering team and development of the system that controls several deep learning models.\n\n[![Image from Gyazo](https://i.gyazo.com/e8f6f32bd9e1e5606d615e68626c4b1a.jpg)](https://gyazo.com/e8f6f32bd9e1e5606d615e68626c4b1a)\n\n## Concept\n\nIn this work, two AI agents repeat both the generation of pictures and the utterance of interpretations alternately. One AI converts the descriptions of the pictures it generates into sentences and speaks them out loud, while the other AI listens, generates the next picture based on it, and speaks in the same way. The newly generated pictures are interpreted and spoken, resulting in a creative Echo.\n\n[![Image from Gyazo](https://i.gyazo.com/91dfa3f470b4f6866f62ab7598baf028.jpg)](https://gyazo.com/91dfa3f470b4f6866f62ab7598baf028)\n\nCurrent image-generating AIs have learned from the pictures and aesthetics that humans have created. The AI-generated pictures are reverberations, or Echoes, of human creativity in the learning data. The generated pictures are eventually disseminated on the Web and used by AI as training data. At this point, although the generated images may seem novel, they can actually be seen as stuck within the Echo that has existed up to that point.\nThis \"inside Echo\" can also be said of us humans. The everyday productions are the result of past creations, just like the Echoes above. With this chain of Echoes, people live in the past and now Echo into the next age.\nHowever, doesn't the next age, or the future, seem to be different from the past age, \"inside Echo\"? Because now there are other things that emit Echoes besides us humans.\nThe AIs here are not only listening to each other's speech, but also to external noises, such as human voices and environmental sounds. It is not clear whether these AIs hate it or admire it, but what is certain is that we can influence each other. And beyond that, a different Echo could resonate.\n\nWhat will be created beyond the point where \"all\" our Echoes resonate with each other?\n\n---\n\nSupervisor: TOKUI Nao  \nTechnical Director: KOBAYASHI Atsuya  \nConcept Director: KOBAYASHI Yuga  \nOriginal Concept: Ryo SIMON  \nLighting: OKAZAKI Keisuke, TAKAISHI Keito, SHIBUYA Kazufumi  \nMachine Learning: ISHII Asuka, SAWA Shoma  \nSound: Ryo SIMON, TAKANASHI Dai, OBARA Kai  \nVisual: TAKAISHI Keito, SHIBUYA Kazufumi, ISHII Asuka, MATSUOKA Yuma  \nConcept: HANDA Sogen, NOBUSUE Ryuku, OKAZAKI Keisuke, INOUE Takumi  \nSupport: NARUSE Santa, KIEU Quoc Thai, SASAKI Yuria  \n","---\ndescription: \"\"\ntitle: \"Orphe Track Effects - prototyping of smart shoes sound effects\"\nslug: orphe\ndate: 2020-4-20 23:20:16\ncategory: Work\ntags: [Music, HCI, NIME]\nkeyVisual: /images/works/orphe.png\n---\n\n[![Image from Gyazo](https://i.gyazo.com/866620a460b6f3efe33fb4876e1cdbec.gif)](https://gyazo.com/866620a460b6f3efe33fb4876e1cdbec)\n\nThis prototype uses [Orphe Track](https://orphe.io/track), which is smart footwear by ORPHE Inc., to capture movements of the foot. I connected the signals from the shoes to Max/MSP application via OSC, to design the audio effect control.\n\n### Electric Guitar Wah\n\n<div class=\"iframe-video-wrapper\">\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/JbzkX24JLvE\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n### Drum Kit Filter\n\n<div class=\"iframe-video-wrapper\">\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/n3EGOPlQG5Q\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n","---\ntitle: \"osc-webapp: simple web controller for receiving osc message from www\"\ndescription: \"\"\nslug: osc-webapp\ndate: 2019-11-16 23:20:16\ncategory: Work\ntags: [Tool, Web]\nkeyVisual: /images/works/osc-webapp.png\n---\n\n\n\n## for music Performance\n\n[![Image from Gyazo](https://i.gyazo.com/cdeca59704c253f33e085146323baebf.jpg)](https://gyazo.com/cdeca59704c253f33e085146323baebf)\n\n[![Image from Gyazo](https://i.gyazo.com/48bbcc7f246d0cc03879addbfb20b2a6.jpg)](https://gyazo.com/48bbcc7f246d0cc03879addbfb20b2a6)\n","---\ndescription: \"\"\ntitle: Search Wikipedia from LINE talk room\nslug: wikipeda-line-chatbot\ndate: 2019-5-12 23:20:16\ncategory: Work\ntags: [Tool]\nkeyVisual: /images/works/wikipeda-line-chatbot.png\n---\n","---\ntitle: \"x-sampling: Sample Sound Transforming with GANs\"\ndescription: \"\"\nslug: x-sampling-gan\ndate: 2019-8-22 23:20:16\ncategory: Work\ntags: [Sound Art, GAN]\nkeyVisual: /images/works/x-sampling-gan.png\n---\n\n# **x-sampling**: Sample sound transforming with GANs\n\nAtsuya Kobayashi\nReo Anzai\n\n![audioeffect](https://i.gyazo.com/13ec66307662856c14ce495b6ec9a907.gif)\n\nSend generator G(z) input vector z (100 dimension) as OSC message. \nOn Max for Live, use `maxpat` file as max audio effect (open `.amxd`).\nthen run `run.py`\n\ncan be used with videos\n\n![video](https://i.gyazo.com/ba4fb1d097976e636553b7db7ca1c323.gif)\n"],"title":"Nextjs Blog Site","description":"A Simple Markdown Blog build with Nextjs."},"__N_SSG":true}